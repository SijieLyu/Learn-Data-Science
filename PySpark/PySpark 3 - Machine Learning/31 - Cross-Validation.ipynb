{"cells":[{"cell_type":"markdown","source":["# Lesson 31 - Cross-Validation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06f99689-d3fe-4a2d-b053-0f0be6c978f2"}}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, expr\n\nfrom pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression \nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator\n\nspark = SparkSession.builder.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e688c4d-2845-4b8b-8cd2-e88e49ab995b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Train/Test Split\nIn the previous lesson, we introduced the train/test split technique for estimating a model's out-of-sample performance. Using this approach, we first split the labeled data into two sets, called the training set and the test set. We train our model on the training set, and then score it on the test set. The score calculated on the test set provides us with an estimate of how the model will perform on out-of-sample data. \n\nThis approach is used often in practice, but it does have certain flaws. The most serious of which is that an estimate generated in this way can be highly dependent on exactly which observations were randomly selected to compose the test set. As a result, estimates created using this approach can have a high variance. \n\nLet's illustrate this idea with an example. Suppose that two data scientists are working with the same dataset. They each decide to randomly set aside 20% of the observations in the dataset for testing and will then train a model on the remaining 80% of the data. The type of model used by the two data scientists will be exactly the same, but the test sets that they each randomly select will be different. They both score their trained models on their own version of the test set. Suppose that one data scientist reports a test set accuracy of 85%, while the other data scientist reports a test set accuracy of 67%. These test scores provide VERY different estimates for how well the model will perform on out-of-sample data. \n\nObviously the test accuracies in this example were manufactured to be far apart for the purpose of making a point. But it is not unreasonable to see differences this large, or even larger, in test scores calculated on two different test sets. Although, it should be pointed out that when working with very large datasets, these estimates do tend to be a bit more stable. \n\nWe will now introduce a more sophisticated approach for estimating a model's out of sample performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c879a5b9-7a64-4fcb-88c8-11326e60ed19"}}},{"cell_type":"markdown","source":["## K-Fold Cross-Validation\nK-fold cross-validation is a popular technique for estimating a model's out-of-sample performance. It is similar to, but more sophisticated than, the train/test split approach. The process for scoring a model using K-fold cross-validation is detailed below.\n\n1. We start by randomly splitting the labeled data into K roughly-equal-sized pieces called **folds**.\n2. We then train K versions of the model, each using the same set of hyperparameters.\n3. Each version of the model is trained on K-1 folds and scored on the remaining fold. That is to say, we estimate the out-of-sample score for each model using the single fold on which that particular model was not trained. Each fold is thus used as the test set for exactly one model.\n4. This will result in K out-of-sample estimates. We average these scores together and report that as the cross-validation score for the model.\n5. We then retrain the model on the entire collection of labeled data that is available to us.\nSince the cross-validation score is calculated as the average of several out-of-sample estimates, it tends to provide a more stable estimate of the model's out of sample performance than that obtained using a single test set.\n\nCommon values for K are 3, 5, 10, and n. When K = n, we refer to the technique as leave-one-out cross-validation, or LOOCV.\n\nThe process of performing K-Fold cross-validation is illustrated in the figure below.\n\n![K-Fold CV](https://drbeane.github.io/files/images/417/kfold_cv.jpeg)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f492b58-7666-4ba3-8078-3e800a1a5995"}}},{"cell_type":"markdown","source":["## Load and Explore Data\n\nTo demonstrate the use of cross-validation to estimate out-of-sample performance, we will return to the synthetic dataset introduced in the previous lesson. Recall that the contents of this dataset has been split into two equal-sized parts which are stored in separate CSV files. We will load these datasets into DataFrames named `df1` and `df2` and will then combine these DataFrames into a single DataFrame named `df`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bed6e81f-7c11-4ec1-ac70-64344576fed9"}}},{"cell_type":"code","source":["df_schema = (\n    'c01 DOUBLE, c02 DOUBLE, c03 DOUBLE, c04 DOUBLE, c05 DOUBLE, '\n    'c06 DOUBLE, c07 DOUBLE, c08 DOUBLE, c09 DOUBLE, c10 DOUBLE, '\n    'c11 STRING, c12 STRING, c13 STRING, c14 STRING, c15 STRING, '\n    'c16 STRING, c17 STRING, c18 STRING, c19 STRING, c20 STRING, '\n    'label INTEGER'\n)\n\ndf1 = (\n    spark.read\n    .option('delimiter', ',')\n    .option('header', True)\n    .schema(df_schema)\n    .csv('/FileStore/tables/synthetic_data_1.csv')\n)\n\ndf2 = (\n    spark.read\n    .option('delimiter', ',')\n    .option('header', True)\n    .schema(df_schema)\n    .csv('/FileStore/tables/synthetic_data_2.csv')\n)\n\ndf = df1.union(df2)\n\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25dc9603-dad1-41e9-94e0-82e53b2df000"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- c01: double (nullable = true)\n |-- c02: double (nullable = true)\n |-- c03: double (nullable = true)\n |-- c04: double (nullable = true)\n |-- c05: double (nullable = true)\n |-- c06: double (nullable = true)\n |-- c07: double (nullable = true)\n |-- c08: double (nullable = true)\n |-- c09: double (nullable = true)\n |-- c10: double (nullable = true)\n |-- c11: string (nullable = true)\n |-- c12: string (nullable = true)\n |-- c13: string (nullable = true)\n |-- c14: string (nullable = true)\n |-- c15: string (nullable = true)\n |-- c16: string (nullable = true)\n |-- c17: string (nullable = true)\n |-- c18: string (nullable = true)\n |-- c19: string (nullable = true)\n |-- c20: string (nullable = true)\n |-- label: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- c01: double (nullable = true)\n-- c02: double (nullable = true)\n-- c03: double (nullable = true)\n-- c04: double (nullable = true)\n-- c05: double (nullable = true)\n-- c06: double (nullable = true)\n-- c07: double (nullable = true)\n-- c08: double (nullable = true)\n-- c09: double (nullable = true)\n-- c10: double (nullable = true)\n-- c11: string (nullable = true)\n-- c12: string (nullable = true)\n-- c13: string (nullable = true)\n-- c14: string (nullable = true)\n-- c15: string (nullable = true)\n-- c16: string (nullable = true)\n-- c17: string (nullable = true)\n-- c18: string (nullable = true)\n-- c19: string (nullable = true)\n-- c20: string (nullable = true)\n-- label: integer (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"981843fc-f4a9-49a5-ab7f-5d6490bd0eff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n| c01| c02| c03| c04| c05| c06| c07| c08| c09| c10|c11|c12|c13|c14|c15|c16|c17|c18|c19|c20|label|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n|4.17| 7.2| 0.0|3.02|1.47|0.92|1.86|3.46|3.97|5.39|  U|  R|  X|  M|  M|  Q|  M|  Y|  N|  Z|    1|\n|4.19|6.85|2.04|8.78|0.27| 6.7|4.17|5.59| 1.4|1.98|  X|  A|  K|  A|  T|  A|  M|  N|  H|  M|    1|\n|8.01|9.68|3.13|6.92|8.76|8.95|0.85|0.39| 1.7|8.78|  H|  Y|  G|  O|  N|  Z|  T|  N|  J|  X|    1|\n|0.98|4.21|9.58|5.33|6.92|3.16|6.87|8.35|0.18| 7.5|  V|  U|  I|  K|  C|  T|  M|  H|  R|  M|    0|\n|9.89|7.48| 2.8|7.89|1.03|4.48|9.09|2.94|2.88| 1.3|  C|  Z|  Z|  E|  Z|  O|  R|  B|  P|  X|    0|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n c01| c02| c03| c04| c05| c06| c07| c08| c09| c10|c11|c12|c13|c14|c15|c16|c17|c18|c19|c20|label|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n4.17| 7.2| 0.0|3.02|1.47|0.92|1.86|3.46|3.97|5.39|  U|  R|  X|  M|  M|  Q|  M|  Y|  N|  Z|    1|\n4.19|6.85|2.04|8.78|0.27| 6.7|4.17|5.59| 1.4|1.98|  X|  A|  K|  A|  T|  A|  M|  N|  H|  M|    1|\n8.01|9.68|3.13|6.92|8.76|8.95|0.85|0.39| 1.7|8.78|  H|  Y|  G|  O|  N|  Z|  T|  N|  J|  X|    1|\n0.98|4.21|9.58|5.33|6.92|3.16|6.87|8.35|0.18| 7.5|  V|  U|  I|  K|  C|  T|  M|  H|  R|  M|    0|\n9.89|7.48| 2.8|7.89|1.03|4.48|9.09|2.94|2.88| 1.3|  C|  Z|  Z|  E|  Z|  O|  R|  B|  P|  X|    0|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\nonly showing top 5 rows\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["N = df.count()\n\nprint(N)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2296015a-cb4c-4a24-b7b5-f5a9c9e51eae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">1000\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1000\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Distribution of Label Values\n\nWe will now determine the distribution of label values in the dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6ec674a-51b7-4b37-8660-a6e267dac50a"}}},{"cell_type":"code","source":["(df.select('label')\n   .groupby('label')\n   .agg(\n       expr('COUNT(*) as count'), \n       expr(f'ROUND(COUNT(*)/{N},4) as prop')\n    ).show()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e688395-db7d-41bb-97b9-59cda03f1175"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----+-----+-----+\n|label|count| prop|\n+-----+-----+-----+\n|    1|  554|0.554|\n|    0|  446|0.446|\n+-----+-----+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+\nlabel|count| prop|\n+-----+-----+-----+\n    1|  554|0.554|\n    0|  446|0.446|\n+-----+-----+-----+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Identify Numerical and Categorical Features\n\nThe first 10 columns of our DataFrames represent numerical features and the next two columns represent categorical features. The last columns represents the label."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e7d9b98-745a-4305-9f27-11a386f9dc99"}}},{"cell_type":"code","source":["num_features = df.columns[:10]\ncat_features = df.columns[10:-1]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a05bbe9-92db-4d0d-ae6d-e3a3f5acd8bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Define Pipeline Stages\n\nWe will now create several stages to perform processing and modeling tasks on our dataset. We will also create an evaluator to calculate the accuracy score for our model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cbc0f30-8228-43f5-9231-1589b0df97e4"}}},{"cell_type":"code","source":["ix_features = [c + '_ix' for c in cat_features]\nvec_features = [c + '_vec' for c in cat_features]\n\nfeature_indexer = StringIndexer(inputCols=cat_features, outputCols=ix_features)\n\nencoder = OneHotEncoder(inputCols=ix_features, outputCols=vec_features, dropLast=False)\n\nassembler = VectorAssembler(inputCols=num_features + vec_features, outputCol='features')\n\nlogreg = LogisticRegression(featuresCol='features', labelCol='label')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a30719a-c0e4-4afd-92b3-c0c0ebeb12b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["accuracy_eval = MulticlassClassificationEvaluator(\n    predictionCol='prediction', labelCol='label', metricName='accuracy')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f685647c-aa34-44fa-b9e0-f2c271a9b0aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Preprocessing\n\nWhen using K-Fold cross validation, our model will be fit K+1 times. We could combine all of the stages for pre-processing and modeling into a pipeline and use that in our cross-validation process, but then each one of these states would have to be fit each of the K+1 times. This would be inefficient and time-consuming. Instead, we will create a pipeline that performs only the preprocessing tasks and will leave the `LogisticRegression` object out of the pipeline. We will then apply the preprocessing pipeline to the data, and will fit only the `LogisticRegression` when performing cross-validation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7eb934ab-f66c-4437-b550-c71d2c0eed6d"}}},{"cell_type":"code","source":["pre_pipeline = Pipeline(stages=[feature_indexer, encoder, assembler]).fit(df)\ntrain = pre_pipeline.transform(df)\ntrain.persist()\n\ntrain.select('features').show(5, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73a36659-aa25-432b-a2a5-6716389c6fac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n|features                                                                                                                                                |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n|(151,[0,1,3,4,5,6,7,8,9,17,29,38,60,70,99,100,120,128,147],[4.17,7.2,3.02,1.47,0.92,1.86,3.46,3.97,5.39,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |\n|(151,[0,1,2,3,4,5,6,7,8,9,19,21,45,56,71,86,100,118,139,150],[4.19,6.85,2.04,8.78,0.27,6.7,4.17,5.59,1.4,1.98,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n|(151,[0,1,2,3,4,5,6,7,8,9,18,20,50,53,76,93,110,118,134,146],[8.01,9.68,3.13,6.92,8.76,8.95,0.85,0.39,1.7,8.78,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n|(151,[0,1,2,3,4,5,6,7,8,9,10,23,43,61,74,96,100,122,129,150],[0.98,4.21,9.58,5.33,6.92,3.16,6.87,8.35,0.18,7.5,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n|(151,[0,1,2,3,4,5,6,7,8,9,15,25,39,58,78,89,101,125,143,146],[9.89,7.48,2.8,7.89,1.03,4.48,9.09,2.94,2.88,1.3,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------------------------------------------------------------------------------------------------------------------+\nfeatures                                                                                                                                                |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n(151,[0,1,3,4,5,6,7,8,9,17,29,38,60,70,99,100,120,128,147],[4.17,7.2,3.02,1.47,0.92,1.86,3.46,3.97,5.39,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |\n(151,[0,1,2,3,4,5,6,7,8,9,19,21,45,56,71,86,100,118,139,150],[4.19,6.85,2.04,8.78,0.27,6.7,4.17,5.59,1.4,1.98,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n(151,[0,1,2,3,4,5,6,7,8,9,18,20,50,53,76,93,110,118,134,146],[8.01,9.68,3.13,6.92,8.76,8.95,0.85,0.39,1.7,8.78,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n(151,[0,1,2,3,4,5,6,7,8,9,10,23,43,61,74,96,100,122,129,150],[0.98,4.21,9.58,5.33,6.92,3.16,6.87,8.35,0.18,7.5,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n(151,[0,1,2,3,4,5,6,7,8,9,15,25,39,58,78,89,101,125,143,146],[9.89,7.48,2.8,7.89,1.03,4.48,9.09,2.94,2.88,1.3,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Train Model \n\nBefore using cross-validation to estimate out-of-sample performance, we will first train the logistic regression model on the entire dataset and will calculate the training accuracy. We do this so that we can compare the results."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2643a25a-9005-41d8-ad62-b7dd0406feba"}}},{"cell_type":"code","source":["logreg_model = logreg.fit(train)\npred = logreg_model.transform(train)\n\nscore = accuracy_eval.evaluate(pred)\n\nprint('Training Score:', score)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b0013cb-0103-4dd4-8dab-5fcf2c8a750d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Training Score: 0.765\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Training Score: 0.765\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Cross-Validation\n\nWe will now use the `CrossValidator` class to perform cross-validation. Notice that when creating the `CrossValidator` instance, we specify values for 5 parameters. \n\n- The `estimator` parameter represents the model being fit and then evaluated on each of the K folds.\n- The `estimatorParamMaps` parameter will be discussed in future lessons. For now, we will simply set it to a list containing an empty dictionary. \n- The `evaluator` parameter represents to `MulticlassClassificationEvaluator` used to score each version of the model. \n- The `numFolds` parameter specifies the number of folds that the data is being split into. \n- The `parallelism` parameter specifies the number of threads to use when performing cross-validation.\n\nAfter creating an instance of the `CrossValidator` class, we must then fit it to the training data. This will return an object of type `CrossValidatorModel`. This new object will contain an `avgMetrics` attribute, which will be a list containing the cross-validation estimate for the model's out-of-sample performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b0c714b-476d-49dd-8a48-9a4014f4f8eb"}}},{"cell_type":"code","source":["cv = CrossValidator(estimator=logreg, estimatorParamMaps=[{}], \n                    evaluator=accuracy_eval, numFolds=10, parallelism=6)\n\ncv_model = cv.fit(train)\n\nprint('\\nCross-Validation Estimate of Out-Of-Sample Performance:', cv_model.avgMetrics[0])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8641759c-919a-495c-81fd-b2668843d858"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n\nCross-Validation Estimate of Out-Of-Sample Performance: 0.6536563450091991\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n\nCross-Validation Estimate of Out-Of-Sample Performance: 0.6536563450091991\n</div>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"31 - Cross-Validation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2377394210003614}},"nbformat":4,"nbformat_minor":0}
