{"cells":[{"cell_type":"markdown","source":["# Lesson 07 - Subsetting and Partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37c15cd6-ff22-4b42-93dd-f22041f429ea"}}},{"cell_type":"code","source":["import numpy as np\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nsc = spark.sparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31c6e17b-da04-4d25-b320-66b368cfebaa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Subsetting RDDs\n\nAs mentioned in the previous lesson, one needs to be careful about using the `collect()` method on a large dataset. If the dataset you are working with is too large to fit in the memory of the cluster node running the driver process, then that node will crash resulting in the termination of the Spark application. In this lesson we will explore techniques that can be used to get a sense as to the contents of an RDD without needing to collect the entire contents onto a single node. \n\nTo demonstrate the use of these methods, we will create an RDD named `letters_rdd` that contains 10,000 string values randomly sampled from the letters `'A'`, `'B'`, `'C'`, `'D'`, `'E'`, and `'F'`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f42c567f-b1c4-4895-b46a-e0dfd7bab391"}}},{"cell_type":"code","source":["letters_arr = np.random.choice(list('ABCDEF'), size=10000)\nletters_rdd = sc.parallelize(letters_arr)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e09dbf88-1147-40c8-be92-2473ab1ac566"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### The take() Method"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"367e6509-2421-4ebf-9405-144b42ef08fb"}}},{"cell_type":"markdown","source":["The **`take()`** action returns the first few elements of an RDD to the driver in the form of a list. This method has a single required parameter named `num` that determines the number of elements to be returned."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c46bc0fa-f1b1-480f-9c0f-5b58b51033ec"}}},{"cell_type":"code","source":["print(letters_rdd.take(12))"],"metadata":{"scrolled":true,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"422f2985-84f7-466b-a841-6c25c9904602"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### The distinct() Method\n\nWe can use the **`distinct()`** transformation to create a new RDD containing only one copy of each element in the source RDD. When we call this method, each executor will determine the unique RDD elements present in the portion of the RDD that they have been assigned. The executors then provide these collections to the driver where the results are merged into the final list. \n\nNote that since `distinct()` is a transformation, we need to `collect()` the returned RDD in order to view it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c88643fb-535c-47f2-8972-6eb1fd88c07b"}}},{"cell_type":"code","source":["unique_rdd = letters_rdd.distinct()\n\nprint(unique_rdd.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19b96b3a-d485-4b66-b49e-74355815b5ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### The sample() Method\n\nWe can use the **`sample()`** transformation to generate a random sample from an RDD. This method has two required parameters:\n* **`withReplacement`**  - Boolean value that controls whether or not the sampling is performed with replacement. \n* **`fraction`** - Sets the probability that any given element will be selected. \n\nNote that since `sample()` is a transformation, we need to `collect()` the returned RDD in order to view it. Run the cell below several times. You should get a different result each time. The expected size of the sample is equal to `fraction` times the size of the RDD, but the actual size of the sample will vary with each call. Change the value of the `fraction` parameter in the code below to see how this affects the result."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a9d7c4d-e01e-41a7-9991-d664e6f463f0"}}},{"cell_type":"code","source":["letters_sample = letters_rdd.sample(withReplacement=False, fraction=0.005)\nprint(letters_sample.count())\nprint(letters_sample.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8c53ea0-0dd4-44cd-adcc-8c689300408f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["When sampling with replacement, Spark considers each element of the RDD and determines at random whether or not that particular value will be in the sample. The probability of an individual value being sampled is equal to `fraction`. The code cell below provides a single example that can be used to explore this idea."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"535523af-220d-4e08-ad3e-31e99fbf8dd9"}}},{"cell_type":"code","source":["my_rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\nsample_rdd = my_rdd.sample(withReplacement=False, fraction=0.3)\n\nprint(sample_rdd.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99b047b6-210f-4d51-833b-267bc60af8a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Partitions\n\nWhen an RDD is created, its contents are split into several partitions which are then distributed across the network with each worker node receiving some of partitions. We can determine the number of partitions that an RDD has been split into by calling the `getNumPartitions()` method of the RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f602d17a-0003-4de9-bb9b-617983e3cee0"}}},{"cell_type":"code","source":["random_arr = np.random.choice(range(100), size=100)\nrandom_rdd = sc.parallelize(random_arr)\n\nprint('Number of Elements in RDD:  ', random_rdd.count())\nprint('Number of Partitions in RDD:', random_rdd.getNumPartitions())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c0827a9-af9d-43af-8af5-c9fa1d8846e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Spark takes care of managing the partitions and we typically do not need to worry about how the RDD elements are distributed across the partitions. However, if we do need to explore this, we can use the `glom()` transformation to view the contents of each partition. This method will return an RDD containing several lists, each of which will represent one of the RDD's partitions.\n\nIn the cell below, we view the elements in the first of the eight partitions that `random_rdd` has been split into."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"595a3f53-1ed9-49c6-af88-0ce8d5847470"}}},{"cell_type":"code","source":["partitions_rdd = random_rdd.glom()\n\nprint('Number of Partititions:     ', partitions_rdd.count())\nprint('Contents of First Partition:', partitions_rdd.take(1)[0])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a050d08b-4dc6-4f42-a9ac-9f9a0e8ba487"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["In the next cell, we will determine the number of elements in each of the 8 partitions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b42772e-8939-4e4f-8f48-4fa31c596ffc"}}},{"cell_type":"code","source":["partitions_list = partitions_rdd.collect()\nfor i, p in enumerate(partitions_list):\n  print(f'Partition Number {i} contains {len(p)} elements.')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edd3741f-8cf4-4004-8b7c-1b20e6f50920"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Notice that the `random_rdd` was split into 8 partitions. The default number of partitions to use for RDDs can be set when creating a Spark session, and can be checked using the `defaultParallelism` attribute of our `sparkContext` object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c519808-937a-4725-9ce0-75592f51fdd7"}}},{"cell_type":"code","source":["print('Default Parallelism:', sc.defaultParallelism)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5702b6e4-c7dd-411c-9102-d91f393f628a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We can control the number of partitions that are to be used for an RDD by setting the optional `numSlices` parameter of the `parallelize()` method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"baf2fedf-1ab8-432f-9ff7-35292602f770"}}},{"cell_type":"code","source":["random_rdd_1 = sc.parallelize(random_arr, numSlices=1)\nrandom_rdd_2 = sc.parallelize(random_arr, numSlices=2)\nrandom_rdd_4 = sc.parallelize(random_arr, numSlices=4)\nrandom_rdd_8 = sc.parallelize(random_arr, numSlices=8)\n\nprint(random_rdd_1.getNumPartitions())\nprint(random_rdd_2.getNumPartitions())\nprint(random_rdd_4.getNumPartitions())\nprint(random_rdd_8.getNumPartitions())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d9ae68c-efd2-4528-a883-2a1ea48f7210"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.1","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"07 - Subsetting and Partitions","dashboards":[{"elements":[],"guid":"d927d7ba-e892-4017-aeae-09746564479d","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"9d8da7e9-60cb-421a-a4e3-1407f8c02846","origId":2377394210003613,"title":"Untitled","width":1024,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2377394210003589}},"nbformat":4,"nbformat_minor":0}
