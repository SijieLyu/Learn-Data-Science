{"cells":[{"cell_type":"markdown","source":["# Lesson 30 - Overfitting"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06f99689-d3fe-4a2d-b053-0f0be6c978f2"}}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, expr\n\nfrom pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\nfrom pyspark.ml.classification import LogisticRegression \nfrom pyspark.ml import Pipeline\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nspark = SparkSession.builder.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e688c4d-2845-4b8b-8cd2-e88e49ab995b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Out-of-Sample Data\n\nUp to this point, when we have measured a model's performance, we have used the training data to calculate some metric (typically accuracy, when working with a classification model). However, when we apply a model in a production setting, we will be applying it to new observations that the model was not trained on, and for which we do not know the true label values. Observations that are not included within the training set are referred to as **out-of-sample** observations. We are typically much more interested in how the model will perform on out-of-sample data than we are in how well the model performs on the training data. \n\nWe will not typically be able to truly measure a model's out-of-sample performance since they would require use to be able to directly measure the model's performance on all possible observations that the model was not trained on, the vast majority of which will likely have labels that are unknown to use. There are techniques that can be used to estimate a model's out-of-sample performance, however. The most obvious technique might be to just use a metric calculated on the training data as an estimate for the same metric as calculated on out-of-sample data. This approach has some serious issues, however."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3c1481f-8e57-4623-aeaf-3b114cfa2c0b"}}},{"cell_type":"markdown","source":["## Overfitting\n\nWhen we train a machine learning model, the training algorithm is designed to optimize performance on the training set. The algorithm will try to take advantage of any correlations between the feature values and label values that might be present within the training set, even if these correlations are the result of noise. The training algorithm might will pick up on patterns that are present within the training data, but that do not generalize to out-of-sample observations. This phenomenon is known as **overfitting**. \n\nAs a result of overfitting, you should always assume that any model metric calculated from the training set will give an overly-optimistic view of how well the model will perform with presented with out-of-sample data. The degree to which to which a training score might overestimate out-of-sample performance varies from dataset to dataset and from model to model. In some situations, the training score might be a good estimate of the model's out-of-sample performance. In other cases, the training score might indicate that a model makes nearly perfect predictions, when in fact the model has nearly no value when it comes to generating predictions on out-of-sample data. \n\nThe key take-away is that you should almost never trust a training score by itself, and should generally use other methods for estimating a model's out-of-sample performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19a7dbaa-1f1f-44a7-88c2-c9be1e9f37c6"}}},{"cell_type":"markdown","source":["## Test Sets\n\nOne common approach to estimating a model's out-of-sample performance is to split the labeled data into two sets: A training set upon which we will train a model and a test set that we will use to evaluate the model. For example, suppose that we randomly select and set aside 20% of our labeled data for testing. We can then training a model on the remaining 80%, and then score that model on the test set, which represents out-of-sample data. This will provide us with an estimate of the model's performance on all out-of-sample observations. After obtaining this estimate, we will typically retrain the model on the entire dataset, since models tend to perform better if they are provided more data to train on. \n\nThis train/test split approach can work reasonably well, but does have some drawbacks. One major flaw in this approach is that the estimates of out-of-sample performance can be highly dependent on exactly what observations are selected for the test set, and so estimates generated in this way can be very volatile. We will discuss this concern in more detail in the next section."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab0c6b82-3dc7-4ffa-bf30-5d87febe35ad"}}},{"cell_type":"markdown","source":["## Load and Explore Data\n\nWe will now demonstrate the phenomenon of overfitting. We will be using a synthetic dataset that has been split into two equal-sized parts. The two parts are stored in separate CSV files named `synthetic_data_1.csv` and `synthetic_data_2.csv`. We will now load both of these datasets into DataFrames named `df1` and `df2`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bed6e81f-7c11-4ec1-ac70-64344576fed9"}}},{"cell_type":"code","source":["df_schema = (\n    'c01 DOUBLE, c02 DOUBLE, c03 DOUBLE, c04 DOUBLE, c05 DOUBLE, '\n    'c06 DOUBLE, c07 DOUBLE, c08 DOUBLE, c09 DOUBLE, c10 DOUBLE, '\n    'c11 STRING, c12 STRING, c13 STRING, c14 STRING, c15 STRING, '\n    'c16 STRING, c17 STRING, c18 STRING, c19 STRING, c20 STRING, '\n    'label INTEGER'\n)\n\ndf1 = (\n    spark.read\n    .option('delimiter', ',')\n    .option('header', True)\n    .schema(df_schema)\n    .csv('/FileStore/tables/synthetic_data_1.csv')\n)\n\ndf2 = (\n    spark.read\n    .option('delimiter', ',')\n    .option('header', True)\n    .schema(df_schema)\n    .csv('/FileStore/tables/synthetic_data_2.csv')\n)\n\ndf1.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25dc9603-dad1-41e9-94e0-82e53b2df000"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- c01: double (nullable = true)\n |-- c02: double (nullable = true)\n |-- c03: double (nullable = true)\n |-- c04: double (nullable = true)\n |-- c05: double (nullable = true)\n |-- c06: double (nullable = true)\n |-- c07: double (nullable = true)\n |-- c08: double (nullable = true)\n |-- c09: double (nullable = true)\n |-- c10: double (nullable = true)\n |-- c11: string (nullable = true)\n |-- c12: string (nullable = true)\n |-- c13: string (nullable = true)\n |-- c14: string (nullable = true)\n |-- c15: string (nullable = true)\n |-- c16: string (nullable = true)\n |-- c17: string (nullable = true)\n |-- c18: string (nullable = true)\n |-- c19: string (nullable = true)\n |-- c20: string (nullable = true)\n |-- label: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- c01: double (nullable = true)\n-- c02: double (nullable = true)\n-- c03: double (nullable = true)\n-- c04: double (nullable = true)\n-- c05: double (nullable = true)\n-- c06: double (nullable = true)\n-- c07: double (nullable = true)\n-- c08: double (nullable = true)\n-- c09: double (nullable = true)\n-- c10: double (nullable = true)\n-- c11: string (nullable = true)\n-- c12: string (nullable = true)\n-- c13: string (nullable = true)\n-- c14: string (nullable = true)\n-- c15: string (nullable = true)\n-- c16: string (nullable = true)\n-- c17: string (nullable = true)\n-- c18: string (nullable = true)\n-- c19: string (nullable = true)\n-- c20: string (nullable = true)\n-- label: integer (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We will now display a few rows of both DataFrames. Note that the structure of the two DataFrames are very similar. It is important to keep in mind that the data contained in this DataFrames were generated using the same process."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71349c27-58b0-4516-bd84-7bfccbe01d51"}}},{"cell_type":"code","source":["df1.show(5)\ndf2.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"981843fc-f4a9-49a5-ab7f-5d6490bd0eff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n| c01| c02| c03| c04| c05| c06| c07| c08| c09| c10|c11|c12|c13|c14|c15|c16|c17|c18|c19|c20|label|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n|4.17| 7.2| 0.0|3.02|1.47|0.92|1.86|3.46|3.97|5.39|  U|  R|  X|  M|  M|  Q|  M|  Y|  N|  Z|    1|\n|4.19|6.85|2.04|8.78|0.27| 6.7|4.17|5.59| 1.4|1.98|  X|  A|  K|  A|  T|  A|  M|  N|  H|  M|    1|\n|8.01|9.68|3.13|6.92|8.76|8.95|0.85|0.39| 1.7|8.78|  H|  Y|  G|  O|  N|  Z|  T|  N|  J|  X|    1|\n|0.98|4.21|9.58|5.33|6.92|3.16|6.87|8.35|0.18| 7.5|  V|  U|  I|  K|  C|  T|  M|  H|  R|  M|    0|\n|9.89|7.48| 2.8|7.89|1.03|4.48|9.09|2.94|2.88| 1.3|  C|  Z|  Z|  E|  Z|  O|  R|  B|  P|  X|    0|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\nonly showing top 5 rows\n\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n| c01| c02| c03| c04| c05| c06| c07| c08| c09| c10|c11|c12|c13|c14|c15|c16|c17|c18|c19|c20|label|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n|0.92|1.86|3.46|3.97|5.39|4.19|6.85|2.04|8.78|0.27|  U|  R|  T|  H|  T|  J|  I|  D|  Q|  X|    0|\n| 6.7|4.17|5.59| 1.4|1.98|8.01|9.68|3.13|6.92|8.76|  C|  H|  L|  Z|  X|  V|  O|  Y|  F|  M|    1|\n|8.95|0.85|0.39| 1.7|8.78|0.98|4.21|9.58|5.33|6.92|  U|  U|  K|  Z|  S|  L|  S|  N|  A|  X|    1|\n|3.16|6.87|8.35|0.18| 7.5|9.89|7.48| 2.8|7.89|1.03|  C|  Z|  Z|  X|  R|  M|  Q|  I|  J|  M|    1|\n|4.48|9.09|2.94|2.88| 1.3|0.19|6.79|2.12|2.66|4.92|  Q|  U|  S|  X|  M|  Y|  Q|  A|  X|  Y|    1|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n c01| c02| c03| c04| c05| c06| c07| c08| c09| c10|c11|c12|c13|c14|c15|c16|c17|c18|c19|c20|label|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n4.17| 7.2| 0.0|3.02|1.47|0.92|1.86|3.46|3.97|5.39|  U|  R|  X|  M|  M|  Q|  M|  Y|  N|  Z|    1|\n4.19|6.85|2.04|8.78|0.27| 6.7|4.17|5.59| 1.4|1.98|  X|  A|  K|  A|  T|  A|  M|  N|  H|  M|    1|\n8.01|9.68|3.13|6.92|8.76|8.95|0.85|0.39| 1.7|8.78|  H|  Y|  G|  O|  N|  Z|  T|  N|  J|  X|    1|\n0.98|4.21|9.58|5.33|6.92|3.16|6.87|8.35|0.18| 7.5|  V|  U|  I|  K|  C|  T|  M|  H|  R|  M|    0|\n9.89|7.48| 2.8|7.89|1.03|4.48|9.09|2.94|2.88| 1.3|  C|  Z|  Z|  E|  Z|  O|  R|  B|  P|  X|    0|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\nonly showing top 5 rows\n\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n c01| c02| c03| c04| c05| c06| c07| c08| c09| c10|c11|c12|c13|c14|c15|c16|c17|c18|c19|c20|label|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\n0.92|1.86|3.46|3.97|5.39|4.19|6.85|2.04|8.78|0.27|  U|  R|  T|  H|  T|  J|  I|  D|  Q|  X|    0|\n 6.7|4.17|5.59| 1.4|1.98|8.01|9.68|3.13|6.92|8.76|  C|  H|  L|  Z|  X|  V|  O|  Y|  F|  M|    1|\n8.95|0.85|0.39| 1.7|8.78|0.98|4.21|9.58|5.33|6.92|  U|  U|  K|  Z|  S|  L|  S|  N|  A|  X|    1|\n3.16|6.87|8.35|0.18| 7.5|9.89|7.48| 2.8|7.89|1.03|  C|  Z|  Z|  X|  R|  M|  Q|  I|  J|  M|    1|\n4.48|9.09|2.94|2.88| 1.3|0.19|6.79|2.12|2.66|4.92|  Q|  U|  S|  X|  M|  Y|  Q|  A|  X|  Y|    1|\n+----+----+----+----+----+----+----+----+----+----+---+---+---+---+---+---+---+---+---+---+-----+\nonly showing top 5 rows\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["N1 = df1.count()\nN2 = df2.count()\n\nprint(N1)\nprint(N2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"397b4b45-56d4-42d4-b804-46cf2678d4a6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">500\n500\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">500\n500\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Distribution of Label Values\n\nWe will now determine the distribution of label values in the two datasets."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6ec674a-51b7-4b37-8660-a6e267dac50a"}}},{"cell_type":"code","source":["(df1.groupby('label')\n    .agg(expr('COUNT(*) as count'), \n         expr(f'ROUND(COUNT(*)/{N1},4) as prop')\n    ).show()\n)\n\n(df2.groupby('label')\n    .agg(\n        expr('COUNT(*) as count'), \n        expr(f'ROUND(COUNT(*)/{N1},4) as prop')\n    ).show()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e688395-db7d-41bb-97b9-59cda03f1175"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----+-----+-----+\n|label|count| prop|\n+-----+-----+-----+\n|    1|  271|0.542|\n|    0|  229|0.458|\n+-----+-----+-----+\n\n+-----+-----+-----+\n|label|count| prop|\n+-----+-----+-----+\n|    1|  283|0.566|\n|    0|  217|0.434|\n+-----+-----+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+\nlabel|count| prop|\n+-----+-----+-----+\n    1|  271|0.542|\n    0|  229|0.458|\n+-----+-----+-----+\n\n+-----+-----+-----+\nlabel|count| prop|\n+-----+-----+-----+\n    1|  283|0.566|\n    0|  217|0.434|\n+-----+-----+-----+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Identify Numerical and Categorical Features\n\nThe first 10 columns of our DataFrames represent numerical features and the next two columns represent categorical features. The last columns represents the label."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e7d9b98-745a-4305-9f27-11a386f9dc99"}}},{"cell_type":"code","source":["num_features = df1.columns[:10]\ncat_features = df1.columns[10:-1]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a05bbe9-92db-4d0d-ae6d-e3a3f5acd8bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Define Pipeline Stages\n\nWe will now create several stages to perform processing and modeling tasks on our dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4cbc0f30-8228-43f5-9231-1589b0df97e4"}}},{"cell_type":"code","source":["ix_features = [c + '_ix' for c in cat_features]\nvec_features = [c + '_vec' for c in cat_features]\n\nfeature_indexer = StringIndexer(inputCols=cat_features, outputCols=ix_features)\n\nencoder = OneHotEncoder(inputCols=ix_features, outputCols=vec_features, dropLast=False)\n\nassembler = VectorAssembler(inputCols=num_features + vec_features, outputCol='features')\n\nlogreg = LogisticRegression(featuresCol='features', labelCol='label')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a30719a-c0e4-4afd-92b3-c0c0ebeb12b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Create and Fit the Pipeline\n\nIn the cell below, we create a pipeline object containing the relevant stages and then fit the pipeline to the data in `df1`. We then use the `transform()` method of the fitted pipeline model to generate predictions for both datasets, `df1` and `df2`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7eb934ab-f66c-4437-b550-c71d2c0eed6d"}}},{"cell_type":"code","source":["model = Pipeline(stages=[feature_indexer, encoder, assembler, logreg]).fit(df1)\n\npred1 = model.transform(df1)\npred2 = model.transform(df2)\n\npred1.select(['probability', 'prediction', 'label']).show(5, truncate=False)\n\npred2.select(['probability', 'prediction', 'label']).show(5, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d3a8ed8-01dd-4727-9cd8-113da24012f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Score the Model\n\nWe will now score our model on the data contained in `df1`, as well as the data contained in `df2`. Note that the model was actually trained on `df1`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c77a1e4-64d0-4ff3-b036-4fa8c1ea542c"}}},{"cell_type":"code","source":["accuracy_eval = MulticlassClassificationEvaluator(\n    predictionCol='prediction', labelCol='label', metricName='accuracy')\n\nscore1 = accuracy_eval.evaluate(pred1)\nscore2 = accuracy_eval.evaluate(pred2)\n\nprint('Training Accuracy (df1):      ', score1)\nprint('Out-of-Sample  Accuracy (df2):', score2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fd3dd89-7d21-4181-9f4a-ca8994a5baa9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Swap Training and Test Sets\n\nAs you can see, the model performed dramatically better on `df1` (which it was trained on) than on `df2`. You might suspect that there might be some characteristic of `df1` or `df2` that simply makes it harder for for a model to perform well on `df2`. To dispel that notion, we will now train the model on `df2` and evaluate it on `df1`. In thos case you see that the model performs significantly better on `df2` than on `df1`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6abd1780-8488-47ef-b1ed-03d67cfcb259"}}},{"cell_type":"code","source":["model = Pipeline(stages=[feature_indexer, encoder, assembler, logreg]).fit(df2)\n\npred1 = model.transform(df1)\npred2 = model.transform(df2)\n\nscore1 = accuracy_eval.evaluate(pred1)\nscore2 = accuracy_eval.evaluate(pred2)\n\nprint('Training Accuracy (df2):     ', score2)\nprint('Out-of-Sample Accuracy (df1):', score1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"033875ee-ccad-4dc7-a74c-ef4d4e8e2d3d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"30 - Overfitting","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2377394210004047}},"nbformat":4,"nbformat_minor":0}
