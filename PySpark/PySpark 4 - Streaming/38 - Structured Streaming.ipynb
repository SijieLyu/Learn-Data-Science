{"cells":[{"cell_type":"markdown","source":["# Lesson 38 - Structured Streaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e0d6dd6-2e50-46f1-9bf4-e47d63282505"}}},{"cell_type":"code","source":["import time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pyspark.sql.functions import col, expr"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83193e6c-465e-45da-ba25-17dadcd20bb3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Retail Data\n\nWe will demonstrate the use of Structured Streaming in Spark by exploring a dataset relating to purchases made with an online retailer. This dataset is stored in several CSV files located in the directory `/FileStore/tables/retail/`. Each file contains information about a single day of purchases. To get a sense as to what our data looks like, and to determine the schema we should use, we will first read the contents of a single file into a static DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96345076-ee52-4676-a6fa-f86456853974"}}},{"cell_type":"code","source":["static_df = (\n    spark.read\n    .option('header', True)\n    .option('inferSchema', True)\n    .csv('/FileStore/tables/retail/2010_12_01.csv')\n)\n\nprint(static_df.count())\n\nretail_schema = static_df.schema\n\nstatic_df.show(5, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d31e409-ed43-4ba8-88a4-489a3872cd47"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Creating a Streaming DataFrame\n\nSpark can read streaming data from a variety of sources. We will discuss some of the available streaming sources in a later lesson, but for now we will consider streams in which new data is periodically written to new files within a directory to which the cluster has access. \n\nThe process of creating a streaming DataFrame from a file source is quite similar to the process used to create a typical static DataFrame from a data file. The primary difference is that where you would use `spark.read` to create a static DataFrame, you would use `spark.readStream` to create a Streaming DataFrame. As is the case when creating a static DataFrame, we can use `option()` to set the delimiter, indicate whether or not the files contain a header, and to set other options. We can also define a schema using `schema()`. The path to the directory that will contain the incoming files can be specified using the `csv()` method.\n\nWhen creating a streaming DataFrame, we can also control how Spark schedules the processing of new data. A **trigger** defines when Spark should check for new data coming into the stream. By default, Spark will trigger the processing of a new micro-batch of data as soon as it finishes process the current micro-batch. This results in the lowest latency, but could incur significant overhead as a result of processing many small micro-batches. Later in this lesson, we discuss how to specify a minimum amount of time that Spark should wait between processing two micro-batches. Increasing the processing time will increase the latency of our streaming application, but could also increase the throughput by decreasing the amount of overhead required.\n\nIn the examples we consider in this course, we will also set `option('maxFilesPerTrigger', 1)`. This tells Spark to process only one file at a time, regardless of how many have been written to the directory since the last trigger. In the examples we consider, there is no external process that is writing files to a directory. All of the files are present in the directory from the beginning. We set this option to simulate the process of files being periodically added to the directory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"79a1d616-c9a6-413d-b514-8c816c720d7b"}}},{"cell_type":"code","source":["retail_stream = (\n    spark.readStream\n    .option('header', True)\n    .option('maxFilesPerTrigger', 1)\n    .schema(retail_schema)\n    .csv('/FileStore/tables/retail/')\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de2314d6-5ac7-430d-80d7-560de4d2930b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We will now print the type of `retail_stream` to see that it is simply a DataFrame. We will also print its `isStreaming` attribute to confirm that Spark understands that this DataFrame was defined using a streaming source."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cdd27374-688d-4a7f-89d3-5fb8408e98f6"}}},{"cell_type":"code","source":["print(type(retail_stream))\nprint(retail_stream.isStreaming)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4222646-385e-4ae8-8fe7-4005dd582e03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Writing Data to a Sink\n\nThere is essentially only one action that can be performed on a streaming DataFrame. That is to start the stream, instructing it to write the results to some destination, or **sink**. This is accomplished by creating a `DataStreamWriter` object. We can create such an object by accessing the `writeStream` attribute of a streaming DataFrame and then setting options to specify how and to where the data is to be written. \n\nWhen creating a `DataStreamWriter`, we need to specify a sink to which the results should be written. We will discuss the types of sinks supported by Structured Streaming in a later lesson. For now, we will simply write the results of our transformations to an in-memory table against which we can submit SQL queries. This is specified using `format('memory')`. We will also need to use `queryName()` to specify the name of the in-memory table to which the results are being written.\n\nNext, we need to specify an output model. Spark Streaming supports three output modes: Append, Complete, and Update. \n\n- **Append Mode.** When using this mode, rows created when processing a new micro-batch will simply be added to the table containing the results. Once a row is written to the output table, it will never to altered. This mode is most useful when we wish to write all of the data from the stream to a table without applying any aggregations. \n- **Complete Mode.** In this mode, all of the records of the output are rewritten to the sink after every new micro-batch. This mode is useful when we are applying grouping and aggregation operations on the contents of the stream. \n- **Update Mode.** When using update model, only the records whose values are affected by the data from the most recent batch are updated. \n\nYou can specify the output mode using the `outputMode()` method, passing it one of the following strings: `'append'`, `'complete'`, or `'update'`. \n\nWe can also use the `trigger()` method to specify that Spark should wait some set amount of time between triggers. For example, if we want to force Structure Streaming to wait at least 10 seconds between micro-batches, we could specify `.trigger(processingTime='10 seconds')` when creating the `DataStreamWriter`.\n\nTo start the stream, you call the `start()` method of the `DataStreamWriter`. This will return a `StreamingQuery` object that can be used to monitor and the status of the stream, and to stop it.Â  We can stop the stream at any point by calling the `stop()` method of the `StreamingQuery` object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f589309a-9b13-4858-8947-e3041df4d173"}}},{"cell_type":"code","source":["writer = (\n    retail_stream\n    .writeStream\n    .trigger(processingTime='2 seconds') \n    .format('memory')\n    .queryName('retail_data')\n    .outputMode('append')\n)\n\nquery = writer.start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4bf9f3e-a3e0-4458-9f2c-6fcdb6fe545d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(type(writer))\nprint(type(query))\nprint(query.isActive)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6002825-c5d6-4bb0-b480-e7f1eea9bd9d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Since we are writing the results of our queries to an in-memory table, we can use `spark.sql` to view the contents of our output at any time when the stream is running."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c82cd23b-041d-4997-9a76-4cd4e90e8847"}}},{"cell_type":"code","source":["for i in range(16):\n    df = spark.sql('SELECT * from retail_data')\n    print(df.count())\n    time.sleep(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6259977-a622-428e-a0ce-202421350917"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["spark.sql('select * from retail_data ORDER BY InvoiceDate DESC').show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a780dec7-99ff-4d8c-838f-27a2d90ae3c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["query.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53b26b1e-4a3d-4aea-bd6b-64c5a776aaf6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Transformations on Streaming DataFrame\n\nIn the previous example, we wrote all of the data received over our stream to a new source in its raw form. However, we will typically want to apply some sort of processing to the streaming data so that we can extract meaningful information from the data being received. One advantage of Spark Structured Streaming is that we can apply most of the standard DataFrame transformations that we are already familiar with to a streaming DataFrame with no modifications. When a transformation is applied to a streaming DataFrame, the resulting DataFrame will also be streaming.Â \n\nFor the sake of completeness of this example, we will recreate our original streaming DataFrame in the cell below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d96e229-c0dd-4e43-9519-ee755dfe2ed5"}}},{"cell_type":"code","source":["retail_stream = (\n    spark.readStream\n    .option('header', True)\n    .option('maxFilesPerTrigger', 1)\n    .schema(retail_schema)\n    .csv('/FileStore/tables/retail/')\n)\n\nprint(retail_stream.isStreaming)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95ca9094-c96b-41f5-87ba-08b62c14e886"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["In the cell below, we will apply several transformations to our streaming DataFrame. Notice that each of these transformations could have been applied to a static version of the data with no changes. Also note that Spark indicates that the DataFrame produced by these transformations is also streaming."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"258f6639-014c-4a7e-a62b-63a4254ffa08"}}},{"cell_type":"code","source":["customer_summary = (\n    retail_stream\n    .withColumn('total_cost', expr('UnitPrice * Quantity'))\n    .groupBy('customerID')\n    .agg(\n        expr('COUNT(*) AS num_purchases'),\n        expr('ROUND(SUM(total_cost),2) AS total_spent'),\n        expr('ROUND(MEAN(total_cost),2) AS avg_spent')\n    )\n    .sort('total_spent', ascending=False)\n)\n\nprint(type(customer_summary))\nprint(customer_summary.isStreaming)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5654ba44-dedb-4c0f-818a-8882384d75cc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We will now create our `DataStreamWriter` and will start the stream. Again, we will write the results of the transformations to an in-memory sink."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89b9cddb-87d7-4dea-ab55-7a744854bdd7"}}},{"cell_type":"code","source":["writer = (\n    customer_summary\n    .writeStream\n    .format('memory')\n    .queryName('customer_summary')\n    .outputMode('complete')\n)\n\nquery = writer.start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf1cc211-d04f-49d9-ac2c-60b24a5f996d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(query.isActive)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb911b0d-d588-4b30-9e04-0244ae6afc72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We will now view the results that have been written to the in-memory sink."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f9c86b4-bb56-488e-b957-99dce46523b4"}}},{"cell_type":"code","source":["print(spark.sql('SELECT * from customer_summary').count())\nspark.sql('SELECT * from customer_summary').show(10, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8bdec4e-a9c9-4605-875d-9f58ba64aa87"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We can use the data that has been written to our sink to create plots. The contents of these plots will change each time these cells are executed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"401bee20-37ba-4b9e-81e6-6900ad0259d1"}}},{"cell_type":"code","source":["pdf = spark.sql('SELECT * from customer_summary WHERE customerID IS NOT NULL').toPandas()\npdf = pdf.sort_values('total_spent', ascending=False)\n\nplt.figure(figsize=[4,6])\nplt.barh(pdf.customerID[20::-1].astype(int).astype(str), \n         pdf.total_spent[20::-1], \n         color='cornflowerblue',\n         edgecolor='k'\n        )\nplt.title('Amount Spent by Customer (Top 20)')\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72212c3d-5c12-4662-ba50-187c4fc42727"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["query.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69f8ab27-d2b2-4b56-888e-67c72a89d2ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Multiple Queries on Same Stream\n\nIt is possible to define multiple `DataStreamWriters` and multiple `StreamingQueries` on the same streaming DataFrame. This can be useful if we want to apply different types of grouping and aggregation transformations to the same stream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5edf3539-9cbd-40e8-87d1-c12294c0d637"}}},{"cell_type":"code","source":["retail_stream = (\n    spark.readStream\n    .option('header', True)\n    .option('maxFilesPerTrigger', 1)\n    .schema(retail_schema)\n    .csv('/FileStore/tables/retail/')\n)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e91d3440-facf-41f9-a7a1-8ff859069b9f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["In the cell below, we define two new DataFrames by applying different sets of transformations to our streaming DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6af2f6d5-3a81-43e8-becd-dd3dffcc2f21"}}},{"cell_type":"code","source":["group_by_product = (\n    retail_stream\n    .groupBy('StockCode')\n    .agg(\n        expr('COUNT(*) AS num_orders'),\n        expr('SUM(Quantity) AS units_sold')\n    )\n    .sort('num_orders', ascending=False)\n)\n\ngroup_by_country = (\n    retail_stream\n    .withColumn('total_cost', expr('UnitPrice * Quantity'))\n    .groupBy('Country')\n    .agg(\n        expr('COUNT(*) AS num_orders'),\n        expr('ROUND(SUM(total_cost),2) AS total_spent'),\n        expr('ROUND(MEAN(total_cost),2) AS avg_spent')\n    )\n    .sort('total_spent', ascending=False)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"371db7d1-ec33-4079-9194-4871b85546e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Next, we will create a different `DataStreamWriter` and `StreamingQuery` object for each of the two transformed DataFrames."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f74e5ca0-fccd-41ec-8c1a-0287d0fb7313"}}},{"cell_type":"code","source":["product_writer = (\n    group_by_product\n    .writeStream\n    .format('memory')\n    .queryName('products')\n    .outputMode('complete')\n)\n\ncountry_writer = (\n    group_by_country\n    .writeStream\n    .format('memory')\n    .queryName('countries')\n    .outputMode('complete')\n)\n\nproduct_query = product_writer.start()\ncountry_query = country_writer.start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b12868e7-f737-4a4b-aba8-8cfe3559ab96"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(product_query.isActive)\nprint(country_query.isActive)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b549353-f1dc-41f9-9840-9c69635c59c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(spark.sql('SELECT * from products').count())\nspark.sql('SELECT * from products ').show(10, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d85a3da5-c6e0-44fd-9b77-66c1bdea4655"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(spark.sql('SELECT * from countries').count())\nspark.sql('SELECT * from countries').show(10, truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"257687da-4bc1-461d-9e53-e9ecc75c18c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["product_query.stop()\ncountry_query.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"34f5cb34-c008-4a81-8bc3-e0cdf4b1cd42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"38 - Structured Streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2377394210004284}},"nbformat":4,"nbformat_minor":0}
