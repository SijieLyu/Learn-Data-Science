{"cells":[{"cell_type":"markdown","source":["#Lesson 05 - Introduction to PySpark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85716ac0-3fe7-4b2b-8450-5eb285d7753d"}}},{"cell_type":"markdown","source":["## Spark Language APIs\n\nApache Spark is written in the Scala programming language, but APIs exist to allow spark applications to be developed using Java, Scala, Python, and R. We will work primarily in Python in this course. The Spark Python API is called **PySpark**."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e38b1f9-a985-46a7-a6f3-c7dd017609bf"}}},{"cell_type":"markdown","source":["## PySpark\n\nIn this lesson, we will provide an introduction to some of the components of a PySpark application. We can import PySpark into our Python session as shown below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dcebcea-c874-4470-bcd3-3c4e51fd5a56"}}},{"cell_type":"code","source":["import pyspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dadac893-2205-4ee6-9240-a7ea2e4d1e1c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## The SparkSession\n\nEvery Spark application has a **SparkSession** object that is created as part of the driver process. The SparkSession instance represents the entry point through which all Spark functionality is accessed. The user issues commands in a Spark application through this SparkSession instance. \n\nThe next cell shows how to create a SparkSession object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"387bf08a-705d-45cf-b85c-ffa6d617009b"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84fe65d1-562e-4d95-a4d6-88feca6a8a80"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We can get information about our Spark session by viewing the `SparkSession` object that we have created."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98767759-7ffd-48c6-99ba-0de12f461aef"}}},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"595b7c15-fecd-449a-82aa-9ab176d12809"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["In the cell below, we will check the type of the object stored in the variable `spark`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec54c591-349e-4c79-bed6-fc85d1d748ec"}}},{"cell_type":"code","source":["print(type(spark))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"62f2584b-eaa9-4a6b-b662-9788b1b330d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["The `SparkSession` object has an attribute that stores the version of Spark that we are running."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"428e3aaa-f29e-466c-9820-dfaa2be97c90"}}},{"cell_type":"code","source":["print(spark.version)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b771a084-d04a-4974-b989-83ed4521f5f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## SparkContext\n\nAs mentioned, the `SparkSession` object is the entry point through which we submit commands to Spark. This object can be used directly to work with DataFrames, which are the primary data type in the SparkSQL component (and will be discussed later in this course). However, the `SparkSession` object also contains entry points for accessing the other components of Spark. The entry points are referred to as **contexts**. The first context we will work with is the **`SparkContext`**, which is associated with the SparkCore component. The SparkContext provides tools for working with Resilient Distributed Datasets, or RDDs. An RDD is the most basic data type introduced by Spark, and all other data types in Spark are built on top of RDDs. \n\nIn the next code cell, we will assign a `SparkContext` object to the variable `sc`. We will then use it do display some information about our Spark environment."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f1beed8-c409-4ef0-9f0b-f40c07d586ba"}}},{"cell_type":"code","source":["sc = spark.sparkContext\n\nprint('Spark Version:', sc.version)\nprint('Spark Mode:', sc.master)\nprint('Spark AppName:', sc.appName)\nprint('Default Parallelism:', sc.defaultParallelism)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cd88537-b5bd-4da5-9449-f1f8a83d7d40"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["In the next lesson, we will begin to explore RDDs and the `SparkContext` object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa08886b-0aed-4b99-9ee4-4dc9934688bb"}}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.1","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"05 - Introduction to PySpark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2377394210003760}},"nbformat":4,"nbformat_minor":0}
