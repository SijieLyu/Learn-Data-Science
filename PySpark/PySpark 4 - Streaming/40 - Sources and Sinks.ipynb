{"cells":[{"cell_type":"markdown","source":["# Lesson 40 - Sources and Sinks"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06f99689-d3fe-4a2d-b053-0f0be6c978f2"}}},{"cell_type":"markdown","source":["## Streaming Sources and Sinks\n\nSpark Structured streaming supports several different types of sources from which to read streaming data, and to which we can write streaming output. We will discuss some of the most common sources and sinks in this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e2d2c5a-48c8-4b47-a19a-968b50699d73"}}},{"cell_type":"markdown","source":["## File Sources and Sinks\nPerhaps the simplest type of source from which to read streaming data is a file source. When using such a source, incoming data is written to a directory by an external process, and then the stream reads and processes the data contained in these new files as they arrive.\n\nSpark also supports writing the output from a stream to files. However, only the append output mode is supported when writing to a file sink. \n\nSpark can read from and write to a a variety of different file formats, including CSV, JSON, and Parquet."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53a4691d-df95-472c-ab43-57feab7954c9"}}},{"cell_type":"markdown","source":["## Apache Kafka\n\nApache Kafka is a framework designed for managing streaming data feeds. Spark Structured Streaming is able to use Kafka as either a source or a sink. All output modes are supported when writing to a Kafka sink. \n\nInformation about integrating Spark and Kafka can be find in the [Structured Streaming + Kafka Integration Guide](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4b5b5ea-bb66-469b-9d21-538c1e6d94ee"}}},{"cell_type":"markdown","source":["## Socket Source\nSpark allows you to send streaming data to the driver over a TCP socket. This source should be used only for testing since the data is sent directly to the driver and socket sources to not provide end-to-end fault-tolerance guarantees."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b89d06c4-831f-46b4-ad63-c346ae8371be"}}},{"cell_type":"markdown","source":["## Memory Sinks\nWhen using a memory sink, the output is stored as an in-memory table. Memory sinks support both append and complete output modes. This type of sink should only be used for testing and debugging purposes since the entire output is written to memory on the driver process, and is not stored in a persistent format."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"490c5ae6-3b87-48bb-aefa-031265ed0af5"}}},{"cell_type":"markdown","source":["## Foreach Sinks\nSpark Structured Streaming provides to option to define a foreach sink that can be used to create a custom data writer to write to destinations that are not already supported by Structured Streaming. Foreach sinks can be also be used to write the output of a data stream to multiple locations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e21e8a0-b8df-499d-ade2-d95fd2946cdd"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"40 - Sources and Sinks","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2377394210004117}},"nbformat":4,"nbformat_minor":0}
