{"cells":[{"cell_type":"markdown","source":["# Lesson 14 - Introduction to DataFrames"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1424d90-a3c4-4d9d-bae8-618913608ec9"}}},{"cell_type":"markdown","source":["## DataFrames\n\nThe Spark **DataFrame** is high-level data structure used for working with structured data. The structure of a DataFrame is similar to that of a table. A DataFrame is a essentially an RDD containing several objects of type **`Row`**. A Row in Spark is an ordered collection of objects. Each Row in a DataFrame must contain the same number of elements. All of the elements at the same position in each of the Rows combine together to form a **`Column`**. Each row in a DataFrame is intended to represent an individual record or observation, and each column is intended to represent a specific value or piece of information that has been recorded for each of the records. \n\nThe Spark DataFrame is inspired by similar data structures in R and in the pandas package for Python. Some key differences between DataFrames in Spark and these other languages are:\n\n1. Spark DataFrames are immutable. \n2. Spark DataFrames are distributed. \n3. Transformations performed on Spark DataFrames are evaluated lazily. \n4. Transformations performed on Spark DataFrames are highly optimized to improve performance."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af07e7cc-a496-461c-b2f2-d3a05791c7f2"}}},{"cell_type":"markdown","source":["## Spark Data Types\n\nEvery column in a DataFrame has a data type associated with it. The data types used by Spark are based on Scala data types and differ somewhat from the data types available in Python. Before continuing on to discuss DataFrames in detail, we need to take a moment to remove some common Spark data types. \n\n* **String Data Types.** Spark provides a `string` data type that is directly equivalent to the python `str` type. \n* **Integer Data Types.** Spark provides several data types for storing integers of different sizes. These include the 1 byte `byte` type, the 2-byte `short` type, the 4-byte `integer` type, and the 8-byte `long` type. In contrast, Python provides a single `int` data type that is capable of scaling based on need. \n* **Floating Point Data Types.** Spark provides two data types for storing floating point numbers. These are the 4-byte `float` type and the 8-byte `double` type. As with integers, Python provides as single `float` data type that scales according to need. \n\nYou can find more information about Spark data types here: [Spark Documentation: Spark Data Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f097584e-c01a-4f70-9393-818dbadf348a"}}},{"cell_type":"markdown","source":["## The SparkSession\n\nThe `SparkSession` object is the primary entry point for working with structured data. The tools that Spark provides specifically for working with DataFrames can be accessed through the `SparkSession`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc9002bc-c009-4b51-81e2-f60ce0c87b84"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27ab8245-2964-42c0-9648-9aa0c7884b20"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Creating a DataFrame from an RDD or List\n\nWe can use the `createDataFrame()` method of the `SparkSession` object to create a DataFrame from an in-memory object such as a list of lists or a pandas DataFrame. This is illustrated with a small example in the following cell.\n\nNote that Spark DataFrames have a `collect()` method, just like RDDs. When we call this method on our newly constructed DataFrame and print each result, we can see the the DataFrame consists of several `Row` objects."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4dcfdd6-2a68-4cd5-8ff9-8cf22c592b3f"}}},{"cell_type":"code","source":["employees_list = [\n  ['Mary', 43, 15.6],\n  ['John', 56, 13.7],\n  ['Kent', 28, 16.2],\n  ['Rose', 34, 16.2],\n  ['Lona', 52, 16.2],\n]\n\nemployees_df = spark.createDataFrame(employees_list)\n\nfor row in employees_df.collect():\n  print(row)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a3e29d8-2c91-49af-8892-bcc9a7a5ca97"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Row(_1=&#39;Mary&#39;, _2=43, _3=15.6)\nRow(_1=&#39;John&#39;, _2=56, _3=13.7)\nRow(_1=&#39;Kent&#39;, _2=28, _3=16.2)\nRow(_1=&#39;Rose&#39;, _2=34, _3=16.2)\nRow(_1=&#39;Lona&#39;, _2=52, _3=16.2)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Row(_1=&#39;Mary&#39;, _2=43, _3=15.6)\nRow(_1=&#39;John&#39;, _2=56, _3=13.7)\nRow(_1=&#39;Kent&#39;, _2=28, _3=16.2)\nRow(_1=&#39;Rose&#39;, _2=34, _3=16.2)\nRow(_1=&#39;Lona&#39;, _2=52, _3=16.2)\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Schemas\n\nEvery Spark DataFrame comes with a **schema** which defines the structure of the DataFrame by specifying a name and data type for each of the columns in the DataFrame. We can view a DataFrame's schema calling its `printSchema()` method."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0096932a-dc9c-472f-b76a-4e30977a49e7"}}},{"cell_type":"code","source":["employees_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89070a01-fccb-4512-ba6a-7315a9a03e2b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _1: string (nullable = true)\n |-- _2: long (nullable = true)\n |-- _3: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _1: string (nullable = true)\n-- _2: long (nullable = true)\n-- _3: double (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Notice the expressions **`_1`**, **`_2`**, and **`_3`** in the output above. These are the names that Spark has assigned to the three columns in our DataFrame. In just a bit, we will see how we can set these names ourselves when creating the DataFrame. \n\nFollowing the name of each column, you will see the Spark data type that has been assigned to that column. These are, in order, `string`, `long`, and `double`. The expression `(nullable = true)` indicates that each of these columns are allowed to contain missing values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64bd2af5-b47b-4ada-8117-d351ee6185c5"}}},{"cell_type":"markdown","source":["### Assigning a Custom Schema\n\nIn the example above, Spark assigned the names `_1`, `_2`, and `_3` to the columns of our DataFrame. If we wish to specify our own column names, or would like to have control over the DataTypes being assigned to the columns, we can create a custom schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab410bf4-3deb-457a-8a5e-6c238c519bc4"}}},{"cell_type":"markdown","source":["### Schema Classes\n\nThere are two commonly-used approaches to created a schema. The first technique that we discuss requires several classes from the `pyspark.sql.types` module. In particular, we need to import the `StructType` class, the `StructField` class, and a specific class assocated with each data type that appears within our columns. Some examples of these data type classes are `StringType`, `IntegerType`, `LongType`, and `DoubleType`. \n\nEvery schema is represented by an element of the `StructType` class. Each column is represented by a `StructField` instance. When creating a `StructField` instance, we have to provide three arguments. These are a string representing the name of the column, a class representing the data type associated with the column, and a boolean value indicating whether or not the column can accept null values. \n\nIn the cell below, we illustrate the process of creating a custom schema by performing the following steps:\n\n1. We import all of the relevant classes. \n2. We create the schema as an instance of the `StructType` class. \n3. We provide the schema as an argument when calling `createDataFrame()`. \n4. We use `printSchema()` to display the schema for the DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d1e83bd-7457-4bb7-9134-24141cc01713"}}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, LongType\n\nmy_schema = StructType([\n  StructField('Name', StringType(), True),\n  StructField('Age', IntegerType(), True),\n  StructField('Rate', DoubleType(), True) \n])\n\nemployees_df = spark.createDataFrame(employees_list, schema=my_schema)\n\nemployees_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58767e55-f41c-46ad-a3ac-d382bfd4d063"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Rate: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Name: string (nullable = true)\n-- Age: integer (nullable = true)\n-- Rate: double (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### DDL Strings\n\nAn alternate approach to creating a schema is to use a **Data Definition Language (DDL) String**. A DDL string is simply a Python string that states the name and data type for each column. The name and data type are separated by spaces, and the information for each column is separated by a comma. This approach is typically much quicker and more concise than the `StructType` approach to defining a schema, and it requires no special import statements. \n\nWe illustrate how to define a schema using a DDL string in the cell below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38e56a8e-2bf6-4846-8cf5-4ebf121d007c"}}},{"cell_type":"code","source":["my_schema = 'Name STRING, Age INTEGER, Rate DOUBLE'\n\nemployees_df = spark.createDataFrame(employees_list, schema=my_schema)\n\nemployees_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d6050cb-a8f3-4c4f-aa94-07281e69a1b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Rate: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Name: string (nullable = true)\n-- Age: integer (nullable = true)\n-- Rate: double (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Column Names\n\nEvery DataFrame has a `columns` attribute that contains a lists of the names of the columns in that DataFrame, as well as a `dtypes` attribute that contains a list of tuples containg the names and data types for each of the columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61c34263-9d62-479f-adb1-6f635560b3c7"}}},{"cell_type":"code","source":["print(employees_df.columns)\nprint(employees_df.dtypes)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7c300159-6a73-44c5-9197-13f7fc198e7e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;Name&#39;, &#39;Age&#39;, &#39;Rate&#39;]\n[(&#39;Name&#39;, &#39;string&#39;), (&#39;Age&#39;, &#39;int&#39;), (&#39;Rate&#39;, &#39;double&#39;)]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;Name&#39;, &#39;Age&#39;, &#39;Rate&#39;]\n[(&#39;Name&#39;, &#39;string&#39;), (&#39;Age&#39;, &#39;int&#39;), (&#39;Rate&#39;, &#39;double&#39;)]\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Displaying DataFrames\n\nThere are several tools that can be used to display the contents of a DataFrame. The first one we will demonstrate is the `show()` DataFrame method. This method displays the first several rows of the DataFrame. By default, 20 rows are displayed, but we can ask for `n` rows to be displayed using `show(n)`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e376e2b-1f4b-4184-9680-8e222a9c19e6"}}},{"cell_type":"code","source":["employees_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6df493de-5197-452e-87dc-e3c45b3117f0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----+---+----+\n|Name|Age|Rate|\n+----+---+----+\n|Mary| 43|15.6|\n|John| 56|13.7|\n|Kent| 28|16.2|\n|Rose| 34|16.2|\n|Lona| 52|16.2|\n+----+---+----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+----+\nName|Age|Rate|\n+----+---+----+\nMary| 43|15.6|\nJohn| 56|13.7|\nKent| 28|16.2|\nRose| 34|16.2|\nLona| 52|16.2|\n+----+---+----+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["DataBricks provides a `display()` function that produces an interactive display of the contents of a DataFrame, or to generate plots from a DataFrame. Note that this tool is specific to DataBricks."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d126bac6-d928-4773-b85e-f286edfa3103"}}},{"cell_type":"code","source":["display(employees_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a8f7239-92ba-4d79-a50b-0c876966c51d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Mary",43,15.6],["John",56,13.7],["Kent",28,16.2],["Rose",34,16.2],["Lona",52,16.2]],"plotOptions":{"displayType":"table","customPlotOptions":{"plotlyBar":[{"key":"grouped","value":true},{"key":"stacked","value":false},{"key":"100_stacked","value":false}],"scatterPlot":[{"key":"loess","value":false},{"key":"bandwidth","value":"0.3"}]},"pivotColumns":[],"pivotAggregation":"sum","xColumns":[],"yColumns":["Rate","Age"]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Name","type":"\"string\"","metadata":"{}"},{"name":"Age","type":"\"integer\"","metadata":"{}"},{"name":"Rate","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th><th>Rate</th></tr></thead><tbody><tr><td>Mary</td><td>43</td><td>15.6</td></tr><tr><td>John</td><td>56</td><td>13.7</td></tr><tr><td>Kent</td><td>28</td><td>16.2</td></tr><tr><td>Rose</td><td>34</td><td>16.2</td></tr><tr><td>Lona</td><td>52</td><td>16.2</td></tr></tbody></table></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We can convert a Spark DataFrame into a pandas DataFrame using `toPandas()`. Since pandas DataFrames are not distributed, using this method will cause all of the contents of the Dataframe to be loaded into memory on the node running the driver process. If the dataset is too large to fit in that machine's memory, then it will likely crash the node."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"810a712e-bf17-4624-9641-7d465a7a45b9"}}},{"cell_type":"code","source":["employees_pdf = employees_df.toPandas()\nemployees_pdf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3582dd6e-12a2-4d25-bc8f-60be15689516"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: </div>"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Age</th>\n      <th>Rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mary</td>\n      <td>43</td>\n      <td>15.6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>John</td>\n      <td>56</td>\n      <td>13.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kent</td>\n      <td>28</td>\n      <td>16.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Rose</td>\n      <td>34</td>\n      <td>16.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lona</td>\n      <td>52</td>\n      <td>16.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Age</th>\n      <th>Rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mary</td>\n      <td>43</td>\n      <td>15.6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>John</td>\n      <td>56</td>\n      <td>13.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kent</td>\n      <td>28</td>\n      <td>16.2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Rose</td>\n      <td>34</td>\n      <td>16.2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Lona</td>\n      <td>52</td>\n      <td>16.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Reading Data from a File\n\nWe can create DataFrames from external files using Spark's **Read API**. This API is accessed through the `spark.read` object, which is of type `DataFrameReader`. We can use the `option()` method of `spark.read` to customize the how the file is read into the DataFrame. The application of this method inlcude setting the delimiter for the data file and specifying if the file contains a header. We can use the `schema()` method to provide the schema to be used for the new DataFrame. Finally, the `csv()` method is used to provide the path to the data file being read. \n\nIn the cell below, we use will create a DataFrame using the gapminder dataset. This dataset is stored in the tab-separated file `FileStore/tables/gapminder.txt`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"461ccc96-d49b-45d4-93f4-2f801a8224ef"}}},{"cell_type":"code","source":["gm_schema = (\n    'country STRING, year INTEGER, continent STRING, population INTEGER, '\n    'life_exp DOUBLE, gdp_per_cap INTEGER, gini DOUBLE'\n)\n\ngm_df = (\n    spark.read\n    .option('delimiter', '\\t')\n    .option('header', True)\n    .schema(gm_schema)\n    .csv('/FileStore/tables/gapminder_data.txt')\n)\n    \ngm_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6381aea3-10be-4c3b-99c0-b8564a3ff906"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- country: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- continent: string (nullable = true)\n |-- population: integer (nullable = true)\n |-- life_exp: double (nullable = true)\n |-- gdp_per_cap: integer (nullable = true)\n |-- gini: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- country: string (nullable = true)\n-- year: integer (nullable = true)\n-- continent: string (nullable = true)\n-- population: integer (nullable = true)\n-- life_exp: double (nullable = true)\n-- gdp_per_cap: integer (nullable = true)\n-- gini: double (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We will now display the first several rows of the DataFrame we just created."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa2fe16b-e925-43bc-92d3-afaec6e984d5"}}},{"cell_type":"code","source":["gm_df.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a80dae6-fed4-4774-99af-196b6b4d78a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------------------+----+---------+----------+--------+-----------+----+\n|            country|year|continent|population|life_exp|gdp_per_cap|gini|\n+-------------------+----+---------+----------+--------+-----------+----+\n|        Afghanistan|1800|     asia|   3280000|    28.2|        603|30.5|\n|            Albania|1800|   europe|    410000|    35.4|        667|38.9|\n|            Algeria|1800|   africa|   2500000|    28.8|        715|56.2|\n|             Angola|1800|   africa|   1570000|    27.0|        618|57.2|\n|Antigua and Barbuda|1800| americas|     37000|    33.5|        757|40.0|\n|          Argentina|1800| americas|    534000|    33.2|       1510|47.7|\n|            Armenia|1800|   europe|    413000|    34.0|        514|31.5|\n|          Australia|1800|     asia|    351000|    34.0|        814|38.7|\n|            Austria|1800|   europe|   3210000|    34.4|       1850|33.4|\n|         Azerbaijan|1800|   europe|    880000|    29.2|        775|70.5|\n+-------------------+----+---------+----------+--------+-----------+----+\nonly showing top 10 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+----+---------+----------+--------+-----------+----+\n            country|year|continent|population|life_exp|gdp_per_cap|gini|\n+-------------------+----+---------+----------+--------+-----------+----+\n        Afghanistan|1800|     asia|   3280000|    28.2|        603|30.5|\n            Albania|1800|   europe|    410000|    35.4|        667|38.9|\n            Algeria|1800|   africa|   2500000|    28.8|        715|56.2|\n             Angola|1800|   africa|   1570000|    27.0|        618|57.2|\nAntigua and Barbuda|1800| americas|     37000|    33.5|        757|40.0|\n          Argentina|1800| americas|    534000|    33.2|       1510|47.7|\n            Armenia|1800|   europe|    413000|    34.0|        514|31.5|\n          Australia|1800|     asia|    351000|    34.0|        814|38.7|\n            Austria|1800|   europe|   3210000|    34.4|       1850|33.4|\n         Azerbaijan|1800|   europe|    880000|    29.2|        775|70.5|\n+-------------------+----+---------+----------+--------+-----------+----+\nonly showing top 10 rows\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Inferring the Schema\n\nThe Spark Read API provides use with the option to allow Spark to scan the dataset to infer the appropriate schema rather than requiring us to specify the schema. This can be set using `option('inferSchema', True)`. Inferring the schema might save us from having to create our own `StructType` or DDL string, but it is less efficient than these other approaches since it requires Spark to scan the dataset in order to determine the correct data types. In it generally better for you to provide Spark with the desired schema information rather than asking Spark to infer it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e97b87a7-09a6-48a5-8f81-fe3bead4a83b"}}},{"cell_type":"code","source":["gm_df = (\n    spark.read\n    .option('delimiter', '\\t')\n    .option('header', True)\n    .option('inferSchema', True)\n    .csv('/FileStore/tables/gapminder_data.txt')\n)\n    \ngm_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33310726-30c6-410b-bc33-9aef9d90d870"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- country: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- continent: string (nullable = true)\n |-- population: integer (nullable = true)\n |-- life_exp: double (nullable = true)\n |-- gdp_per_cap: integer (nullable = true)\n |-- gini: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- country: string (nullable = true)\n-- year: integer (nullable = true)\n-- continent: string (nullable = true)\n-- population: integer (nullable = true)\n-- life_exp: double (nullable = true)\n-- gdp_per_cap: integer (nullable = true)\n-- gini: double (nullable = true)\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"80c25eab-e316-4182-8ad4-06ee2a838351"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"14 - Intro to DataFrames","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2377394210003347}},"nbformat":4,"nbformat_minor":0}
