{"cells":[{"cell_type":"markdown","source":["# Lesson 17 - Column Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1424d90-a3c4-4d9d-bae8-618913608ec9"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, expr\nfrom pyspark.sql.types import StringType, IntegerType\n\nspark = SparkSession.builder.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"763a92ec-7d10-47b2-9997-4fa2b78cbeb6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Built-In Functions\n\nSpark provides several built-in functions that can be applied to column objects. These functions are stored in the [`pyspark.sql.functions`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#module-pyspark.sql.functions) module, which is often imported under the alias `F`, as shown in the the import statement below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f07bb7f-94a7-4dea-92b7-f218b42795b3"}}},{"cell_type":"code","source":["import pyspark.sql.functions as F"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8944695-3829-4b6c-a953-0fbebaeccb63"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["This module contains both element-wise functions and aggregation functions that can be applied to columns. We will discuss both types of functions in this lesson, and will provide a few examples of each type. A complete list of available functions can be found in the [Apache Spark Documentation](https://spark.apache.org/docs/3.0.1/api/sql/#lower)\n\nWe will create a small DataFrame to use in illustrating the concepts introduced in this lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca711e72-9aac-4022-8c5d-d90da7af52ca"}}},{"cell_type":"code","source":["my_schema = 'name STRING, x1 DOUBLE, x2 INTEGER'\n\ndata = [\n  ['Emma White', 5.2, 215],\n  ['Art Brown', 4.1, 473],\n  ['Carly Black', 3.7, 260],\n  ['Beth Green', 4.5, 303],\n  ['Dan Gray', 2.9, 185]\n]\n\ndf = spark.createDataFrame(data, schema=my_schema)\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8c8b126-afbe-4ce2-8d5a-406ff4be274a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+---+---+\n|       name| x1| x2|\n+-----------+---+---+\n| Emma White|5.2|215|\n|  Art Brown|4.1|473|\n|Carly Black|3.7|260|\n| Beth Green|4.5|303|\n|   Dan Gray|2.9|185|\n+-----------+---+---+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---+---+\n       name| x1| x2|\n+-----------+---+---+\n Emma White|5.2|215|\n  Art Brown|4.1|473|\nCarly Black|3.7|260|\n Beth Green|4.5|303|\n   Dan Gray|2.9|185|\n+-----------+---+---+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Element-Wise Functions\n\n\nElement-wise functions represent an operation that is to be applied independently to every element of a column in a DataFrame. In the cell below, we provide an example that makes use of the following element-wise functions:\n\n* `F.upper()` - This function can be applied to `string` columns. It returns a column in which all of the strings have been converted to upper case. \n* `F.length()` - This function can be applied to `string` column. It returns a column containing the lengths of the strings in the original column. \n* `F.exp()` - This function accepts a column of numerical values and applies the natural exponential function to each value in the column.\n* `F.log()` - This function accepts a column of numerical values and applies the natural logarithm function to each value in the column.\n\nNote that in the example below, we use `alias()` to name the newly created column. If we did not do this then the name of the new column would be an unsightly string representing the operation used to create the column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37d18537-41ca-4930-affd-a521da4c5ac1"}}},{"cell_type":"code","source":["df.select(\n    F.upper(col('name')).alias('name_upper'),\n    F.length(col('name')).alias('name_length'),\n    F.exp(col('x1')).alias('exp_x1'),\n    F.log(col('x2')).alias('log_x2')\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9612833-755c-4d90-9eed-af71da7e1ed4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+-----------+------------------+------------------+\n| name_upper|name_length|            exp_x1|            log_x2|\n+-----------+-----------+------------------+------------------+\n| EMMA WHITE|         10|181.27224187515122|5.3706380281276624|\n|  ART BROWN|          9| 60.34028759736195| 6.159095388491933|\n|CARLY BLACK|         11|  40.4473043600674| 5.560681631015528|\n| BETH GREEN|         10| 90.01713130052181| 5.713732805509369|\n|   DAN GRAY|          8| 18.17414536944306| 5.220355825078325|\n+-----------+-----------+------------------+------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-----------+------------------+------------------+\n name_upper|name_length|            exp_x1|            log_x2|\n+-----------+-----------+------------------+------------------+\n EMMA WHITE|         10|181.27224187515122|5.3706380281276624|\n  ART BROWN|          9| 60.34028759736195| 6.159095388491933|\nCARLY BLACK|         11|  40.4473043600674| 5.560681631015528|\n BETH GREEN|         10| 90.01713130052181| 5.713732805509369|\n   DAN GRAY|          8| 18.17414536944306| 5.220355825078325|\n+-----------+-----------+------------------+------------------+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Rounding\n\nThe built-in `F.round()` function can be used to round values in a numerical column. It accepts two arguments. The first is the column to which the function is being applied and the second is the number of decimal deigits to which the values should be rounded. We will modify our previous example by rounding the third and fourth columns to 2 and 4 decimal places, respectively."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a04d6ea-c5bb-47de-aa92-20f94145d687"}}},{"cell_type":"code","source":["df.select(\n    F.upper(col('name')).alias('name_upper'),\n    F.length(col('name')).alias('name_length'),\n    F.round(F.exp(col('x1')),2).alias('exp_x1'),\n    F.round(F.log(col('x2')),4).alias('log_x2')\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f43330a1-3674-4f75-bee9-8a4a97905fda"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+-----------+------+------+\n| name_upper|name_length|exp_x1|log_x2|\n+-----------+-----------+------+------+\n| EMMA WHITE|         10|181.27|5.3706|\n|  ART BROWN|          9| 60.34|6.1591|\n|CARLY BLACK|         11| 40.45|5.5607|\n| BETH GREEN|         10| 90.02|5.7137|\n|   DAN GRAY|          8| 18.17|5.2204|\n+-----------+-----------+------+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-----------+------+------+\n name_upper|name_length|exp_x1|log_x2|\n+-----------+-----------+------+------+\n EMMA WHITE|         10|181.27|5.3706|\n  ART BROWN|          9| 60.34|6.1591|\nCARLY BLACK|         11| 40.45|5.5607|\n BETH GREEN|         10| 90.02|5.7137|\n   DAN GRAY|          8| 18.17|5.2204|\n+-----------+-----------+------+------+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Using Functions in SQL Expression Strings\n\nWe can also use `expr()` and SQL expression strings to apply functions to columns. When doing so, the expression string is sent to Spark where it is parsed and executed. Since the strings gets parsed on the backend by Spark, we do not need to import any modules or functions when using this approach. Every function found in `pyspark.sql.functions` has an SQL equivalent of the same name. Note that SQL functions is not case-sensitive and you will often see them written in all-caps.\n\nThe cell below illustrates how to use `expr()` and SQL expression strings to recreate the previous example that we considered."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e704444-525e-4510-af31-750447cc9354"}}},{"cell_type":"code","source":["df.select(\n    expr('UPPER(name) AS name_upper'),\n    expr('LENGTH(name) AS name_length'),\n    expr('ROUND(EXP(x1), 2) AS exp_x1'),\n    expr('ROUND(LOG(x2), 4) AS log_x2')\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4777a44a-87b6-4a13-b9d9-cdfa83303a6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Aggregation Functions\n\nAggregation functions combine all of the entries in a column into a single value, and can be applied using the `select()` method. In the example below, we will illustrate the use of the following aggregation functions:\n\n* `F.sum()` - Returns the sum of the elements in a column. \n* `F.mean()` - Returns the arithmetic mean of the elements in a column. \n* `F.stddev()` - Returns the standard deviation of the elements in a column. \n* `F.min()` - Returns the minimum of the elements in a column. \n* `F.max()` - Returns the maximum of the elements in a column. \n\nAgain, we will use `alias()` to assign friendly names to the new columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c60d66c7-8424-43e5-8953-e9fb98d9bbf2"}}},{"cell_type":"code","source":["df.select(\n    F.sum(col('x2')).alias('sum_x2'),\n    F.mean(col('x2')).alias('mean_x2'),\n    F.stddev(col('x2')).alias('stddev_x2'),\n    F.min(col('x2')).alias('min_x2'),\n    F.max(col('x2')).alias('max_x2')\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5859cf79-d9fb-4cf1-a3e9-a0837dda3028"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------+-------+------------------+------+------+\n|sum_x2|mean_x2|         stddev_x2|min_x2|max_x2|\n+------+-------+------------------+------+------+\n|  1436|  287.2|113.10260828115327|   185|   473|\n+------+-------+------------------+------+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-------+------------------+------+------+\nsum_x2|mean_x2|         stddev_x2|min_x2|max_x2|\n+------+-------+------------------+------+------+\n  1436|  287.2|113.10260828115327|   185|   473|\n+------+-------+------------------+------+------+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We will now recreate the results above, this time using SQL expressing strings."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47d17603-c60e-4b78-8de9-42adfc524aff"}}},{"cell_type":"code","source":["df.select(\n    expr('SUM(x2) AS sum_x2'),\n    expr('MEAN(x2) AS mean_x2'),\n    expr('STDDEV(x2) AS stddev_x2'),\n    expr('MIN(x2) AS min_x2'),\n    expr('MAX(x2) AS max_x2')\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4564d652-ec75-4868-b360-9c3aa3db0a03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------+-------+------------------+------+------+\n|sum_x2|mean_x2|         stddev_x2|min_x2|max_x2|\n+------+-------+------------------+------+------+\n|  1436|  287.2|113.10260828115327|   185|   473|\n+------+-------+------------------+------+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+-------+------------------+------+------+\nsum_x2|mean_x2|         stddev_x2|min_x2|max_x2|\n+------+-------+------------------+------+------+\n  1436|  287.2|113.10260828115327|   185|   473|\n+------+-------+------------------+------+------+\n\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Rules for Applying Multiple Column Functions\n\nYou are allowed to include several element-wise functions within a single call to `select()` and you can include several aggregations into a single such call. However, you are not allowed to mix element-wise functions and aggregations within the same call to `select()`. The column returned by an element-wise function will contain one entry for every row in the original DataFrame, while the column returned by an aggregator will contain only a single value representing the aggregated value."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9199b9f-7952-4b9b-aca7-91afd2659207"}}},{"cell_type":"markdown","source":["## User-Defined Functions\n\nThere are occasions when you need to apply a function to a column in a DataFrame, and that function is not provided as a built-in function. In this scenario, you can created a **user-defined function**, or **UDF**. The general process for creating a UDF is to write a Python function that performs the desired operation on a single element, and then use Spark tools to create a column function that applies the Python function to each element in a DataFrame column.\n\nThe details of the approach used to create a UDF vary depending on if we plan to apply the function to a column object created by `col()`, or if we wish to use the UDF inside a SQL expression string provided to `expr()`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0a8ff84-c09e-44d1-a538-3196d09a9b0d"}}},{"cell_type":"markdown","source":["#### Creating a UDF in Python\n\nSuppose that we want to create a function in Python that can be applied to a column object created by `col()`. To do so, we start by writing a Python function whose parameters are assumed to be values selected from one or more columns. For the sake of discussion, let's call this function  my_func. We then use the `F.udf()` function to create a UDF that applies `my_func` to each element of a column. The function `F.udf()` expects two arguments. The first is the function we intend to apply element-wise (`my_func`, in our current example). The second argument is a Spark type object (such as `IntegerType()` or `StringType()`) used to specifiy the type to be used for the column returned by the UDF.\n\nWe will now demonstrate this process by creating a UDF that extracts the last names of individuals in our dataset. We perform the following tasks in the cell below:\n1. We create a Python function that performs the desired operation on strings of the type stored in the `name` column. \n2. We use `F.udf()` to create a function that accepts a column object as an input, and then performs the desired operation on individual strings within that column. \n3. We use `select()` to apply the new UDF to the `name` column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8704c71-f68b-4288-ad5e-21e79ecc0fe3"}}},{"cell_type":"code","source":["def get_last_name(name):\n    tokens = name.split(' ')\n    return tokens[-1]\n\nget_last_name_udf = F.udf(get_last_name, StringType())\n\ndf.select(\n    '*', \n    get_last_name_udf(col('name')).alias('last_name')\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2766aa6-dd1e-434a-9d5e-169513d26727"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["The cell below provides another example of creating a UDF. In this example, we create a UDF that sums the digits of the elements contained within a integer column. We then apply this UDF to the column `x2`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c13efb0e-e10b-40d9-8736-85cd11f2e9bd"}}},{"cell_type":"code","source":["def sum_digits(n):\n    temp = n\n    total = 0\n    while temp > 0:\n        total += temp % 10\n        temp = int(temp / 10)\n    return total\n\nsum_digits_udf = F.udf(sum_digits, IntegerType())\n\ndf.select(\n    '*', \n    sum_digits_udf(col('x2')).alias('digit_sum')\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95119013-e76f-4c70-9a1d-a9cc5ae1905f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Registering UDFs to use in SQL-based Expression Strings\n\nThe UDFs we created in the cells above are Python functions. They accept column objects as input and return column objects as output. When the resulting column objects are provided to `select()` and an action is called, Spark will perform the desired operations. \n\nWe can also create new UDFs directly within the Spark backend using `spark.udf.register()`. This function expects two arguments. The first argument is a string representing the name of the new SQL function we are creating. The second argument is the Python function that that defines the elementwise operation represented by our new function. The newly created function will not be available within Python, but it can be used in SQL-based expression strings provided to `expr()`. \n\nWe illustrate the use of this technique in the cell below. Notice that this approach results in cleaner code, as is often the case when using `expr()`. Also, this technique does not require us to import any special functions or classes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4a3a0dc-18e4-4175-8a26-2ca41884ab9c"}}},{"cell_type":"code","source":["spark.udf.register('GET_LAST_NAME', get_last_name)\nspark.udf.register('SUM_DIGITS', sum_digits)\n\ndf.select(\n    '*', \n    expr('GET_LAST_NAME(name) AS last_name'),\n    expr('SUM_DIGITS(x2) AS digit_sum')\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cfb3dec0-b34f-4869-85c8-b90993272738"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"17 - Column Functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2377394210003943}},"nbformat":4,"nbformat_minor":0}
