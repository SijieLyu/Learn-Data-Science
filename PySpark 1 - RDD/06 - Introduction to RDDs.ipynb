{"cells":[{"cell_type":"markdown","source":["# Lesson 06 - Introduction to RDDs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44a0e52e-f8a9-47cb-a8ca-2a3a119c20a5"}}},{"cell_type":"markdown","source":["## The Resilient Distributed Dataset\n\nThe primary data abstraction in Apache Spark is the **Resilient Distributed Dataset**, or **RDD**. According to the [Apache Spark documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds), an RDD is a \"fault-tolerant collection of elements that can be operated on in parallel\". RDDs are superficially similar to the `list` data type that you may already be familiar with from Python. However, there are several important differences between lists and RDDs. To better understand the important features of an RDD, we will decompose the name of the data type, taking the words in reverse order. \n\n* **Dataset.** An RDD is a collection of values. An individual value contained within an RDD is called an **element** of the RDD. The elements of an RDD can be of any data type and a single RDD can contain elements of many different data types. \n* **Distributed.** When we create an RDD, it is split into several smaller pieces called **partitions**. Spark distributes the partitions of the RDD across the nodes in the cluster. \n* **Resilient.** An RDD is considered \"fault-tolerant\". An RDD is generally able to be fully reconstructed if some of its partitions are damaged or lost as a result of node failures. \n\nAdditionally, it should be noted that RDDs are immutable data types. This means that once created, the contents of an RDD cannot be altered. If we apply a transformation to the contents of an RDD (such as squaring each element), a new RDD is created containing the results of the transformation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5b60506-b898-4106-ae55-c427f43775bc"}}},{"cell_type":"markdown","source":["## Low-Level Data Structure\n\nSpark RDDs are low-level data structures on top of which all higher-level Spark data structures, such as DataFrames, are constructed. In Spark version 1.x, RDDs were the primary data type in Spark. This changed in version 2.x, at which point Spark SQL became the foremost component of Spark and DataFrames became the primary data type. \n\nToday, DataFrames are used much more frequently than RDDs. The DataFrame API in Spark is more highly developed and provides beneath-the-hood optimization techniques that are not available for RDDs. However, there are still times when one might wish to use RDDs, as they allow for more control and customization than DataFrames."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bac9fa5-8f9f-4adf-a8d0-3ab44af8dac9"}}},{"cell_type":"markdown","source":["## Transformations, Actions, and Lazy Evaluation\n\nRDDs come equipped with several methods, which can be grouped into two categories: **transformations** and **actions**. \n\n* A **transformation** is a method that applies some sort of operation to the elements of an RDD returning a new, transformed RDD. \n* An **action** is a method that represents some sort of calculation in which information is returned to the driver. \n\nIn some sense, the core distinction between these two types of operations is that a transformation returns a new RDD, while an action produces output that is not an RDD. For example, an action might display output to the screen, write data to disk, or return a different data type, such as a Python list. \n\nData processing tasks in Spark are performed according to a strategy know as **lazy evaluation** in which tasks are not performed immediately, but are instead postponed until they are required in order to fulfill a request for some specific output. \n\nWhen we call a transformation in Spark, the resulting RDD is not calculated immediately. Instead, the requested calculation is put into a queue, only to be performed when we call an action on the resulting RDD. This evaluation strategy had several benefits. It allows Spark to delay expensive calculations until absolutely necessary. It also allows Spark to optimize a sequence of transformations by combining similar transformations together, or eliminating redundant transformations. We will discuss lazy evaluation in more detail in a later lecture."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b393e4a-8870-4eca-9614-4cd81b801ad0"}}},{"cell_type":"markdown","source":["## The SparkContext\n\nThe **`SparkContext`** object provides an API for creating and working with RDDs. There are multiple ways to create a `SparkContext`, but one is created automatically when we create an instance of the `SparkSession`. In the code cell below, we create a `SparkSession` object and then assign the associated `SparkContext` object to a variable named `sc`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66e7b6db-9bd8-45ae-a092-4d4e40404ca9"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\nsc = spark.sparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f7e259d-a68d-4d10-9c91-3c6da0686dec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We can print the `SparkContext` object to get information about our Spark environment. The `SparkContext` also contains a `version` attribute that stores the version of Spark that we are currently running."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1488787e-d590-44cd-87db-87ea998f89f7"}}},{"cell_type":"code","source":["print(sc)\nprint(sc.version)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c29644fd-38d7-47f0-96ce-0b228a8536b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;SparkContext master=spark://10.215.242.170:7077 appName=Databricks Shell&gt;\n3.0.1\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;SparkContext master=spark://10.215.242.170:7077 appName=Databricks Shell&gt;\n3.0.1\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Creating RDDs\n\nThere are two common ways to create RDDS:\n1. Using the `parallelize()` method of the `SparkContext` object to create an RDD from an in-memory object, such as a list or NumPy array. \n2. Using the `textFile()` method of the `SparkContext` object to create an RDD from a data file.\n\nWe will discuss reading files from external sources later in this lesson. For now, we will focus on using `parallelize` to create RDDs from in-memory collections. To illustrate this process, we will begin by creating a NumPy array consisting of 50 randomly generated integers."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e4c996f9-dac3-4f8b-a203-6f2232dea43c"}}},{"cell_type":"code","source":["import numpy as np\nrandom_array = np.random.choice(range(100), size=50)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c894376e-7b07-4560-8530-494ac14c149e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["In the cell below, we will use the `parallelize()` method to create an RDD from our newly created array. We will also print the type of the RDD to confirm that it is in fact an RDD object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c292646f-98d5-4255-a969-63aa8c5e37fd"}}},{"cell_type":"code","source":["random_rdd = sc.parallelize(random_array)\nprint(type(random_rdd))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d4fb1913-34c1-4387-b2fa-bf5c1f4fc323"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;pyspark.rdd.RDD&#39;&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;pyspark.rdd.RDD&#39;&gt;\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## The collect() Action\n\nSuppose that we would like to view the contents of an RDD. It would be natural to try to place the RDD inside of the `print()` function. Let's see what happens if we do this."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30a26ed1-4545-473a-8cce-6f2714803c0f"}}},{"cell_type":"code","source":["print(random_rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"979a7737-68cc-4999-a6d2-ffd6186c827b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">ParallelCollectionRDD[0] at readRDDFromInputStream at PythonRDD.scala:413\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ParallelCollectionRDD[0] at readRDDFromInputStream at PythonRDD.scala:413\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Notice that the output displayed for the cell is not a squence of 50 random integers. Instead we see some technical information regarding the way the RDD was created. \n\nThe reason why `print(random_rdd)` did not print the contents of the RDD is that the RDD does not actually contain any information yet. As mentioned above, RDDs are evaluated lazily. This means that the contents of an RDD are not actually generated until we call an action that requires those values. The Python `print()` function is not a Spark action. \n\nWe can force the contents of an RDD to be generated by using the `collect()` action. This RDD method tells Spark to calculate the contents of the RDD and then return those contents to the driver in the form of a list. \n\nIn the cell below, we call the `collect()` action on `random_rdd`, and then print the resulting list."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49c0c63b-befc-408c-b4a1-b983a46c693c"}}},{"cell_type":"code","source":["print(random_rdd.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e0ce613-525e-4878-8cd9-11d8c9e38a3b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[30, 67, 47, 38, 10, 48, 34, 31, 67, 4, 82, 71, 14, 80, 16, 93, 69, 30, 29, 16, 42, 21, 11, 18, 56, 65, 1, 57, 94, 75, 90, 23, 19, 8, 30, 24, 96, 45, 96, 97, 82, 5, 52, 99, 81, 58, 70, 72, 91, 18]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[30, 67, 47, 38, 10, 48, 34, 31, 67, 4, 82, 71, 14, 80, 16, 93, 69, 30, 29, 16, 42, 21, 11, 18, 56, 65, 1, 57, 94, 75, 90, 23, 19, 8, 30, 24, 96, 45, 96, 97, 82, 5, 52, 99, 81, 58, 70, 72, 91, 18]\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["print(type(random_rdd.collect()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"540d86ef-4ac0-409b-872c-d48603d8c59a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &#39;list&#39;&gt;\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["You should be very careful about using `collect()` on an RDD containing a very large dataset. When working on a cluster, the contents of an RDD will be split across the nodes in that cluster. By calling the `collect()` method, you are requesting that all of the elements in the RDD be collected onto the driver as an in-memory list. If the resulting list is too large to fit into the memory of the node running the driver, that node will likely crash killing your application. \n\nIn a later lesson we will discuss how to using sampling and subsetting to explore the contents of a large RDD without collecting the entire RDD into memory on the driver."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31116e58-c725-42c5-9b4c-ac200948051f"}}},{"cell_type":"markdown","source":["## Descriptive Statistics\n\nSpark provides several RDD methods for calculating descriptive statistics. These include the following methods: `count()`, `sum()`, `mean()`, `variance()`, `stdev()`, `min()`, and `max()`. Each of these methods represent an RDD action. Note that many of these actions can only be used on RDDs containing numerical values. \n\nWe will use `random_rdd` to demonstrate the use of these actions in the cell below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f2d964c-8ea5-4d90-ace1-e4f107fa9a9f"}}},{"cell_type":"code","source":["print('Count:   ', random_rdd.count())    # Number of elements\nprint('Sum:     ', random_rdd.sum())      # Sum of elements\nprint('Mean:    ', random_rdd.mean())     # Mean (or average)\nprint('Variance:', random_rdd.variance()) # Population Variance\nprint('Std Dev: ', random_rdd.stdev())    # Population Standard Deviation\nprint('Minimum: ', random_rdd.min())      # Minimum (smallest) element\nprint('Maximum: ', random_rdd.max())      # Maximum (largest) element"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a50a66d-d18c-49db-be90-1641b69c9c76"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["You can calculate the count, mean, standard deviation, min, and max of an RDD with a single call to the `stats()` action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7a8a01b-5a86-44ba-85d1-60cabf5f7df6"}}},{"cell_type":"code","source":["print(random_rdd.stats())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b18ee762-de58-4349-8c6d-b0b390c9796d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Reading from a Text File\n\nWe can use the `textFile()` method to create an RDD from a text file stored in a local file system, as long as that file is in a location that is accessible by each of the nodes in the cluster. Each line in the text file will be stored as a single element of the resulting RDD and will be represented as a string within the new RDD.\n\nIn the cell below, we read the contents of a file named `datafile_01.txt`. This file contains a single line of text. That line contains several integer values separated by spaces. Notice that the resulting RDD contains only a single element."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"800aad8c-6b47-49ec-b480-672e2f6a0502"}}},{"cell_type":"code","source":["temp_rdd_1 = sc.textFile('/FileStore/tables/datafile_01.txt')\nprint(temp_rdd_1.count())\nprint(temp_rdd_1.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"779cafcf-9975-429e-a511-c9a6f212d818"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">1\n[&#39;5 7 4 7 8 2 5 6 2 7 5 3&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1\n[&#39;5 7 4 7 8 2 5 6 2 7 5 3&#39;]\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["In the next cell, we read the contents of a file named `datafile_02.txt`. This file contains twelve lines of text. Each line contains a single character representing an integer value. Notice that the RDD created from this file contains twelve elements, and that each of the elements are read in to the RDD as strings."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02f0bb9c-ee8d-4176-b31d-c82329c55a05"}}},{"cell_type":"code","source":["temp_rdd_2 = sc.textFile('/FileStore/tables/datafile_02.txt')\nprint(temp_rdd_2.count())\nprint(temp_rdd_2.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"461dfa99-bafd-415d-8350-06c3dea0acd9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">12\n[&#39;5&#39;, &#39;7&#39;, &#39;4&#39;, &#39;7&#39;, &#39;8&#39;, &#39;2&#39;, &#39;5&#39;, &#39;6&#39;, &#39;2&#39;, &#39;7&#39;, &#39;5&#39;, &#39;3&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">12\n[&#39;5&#39;, &#39;7&#39;, &#39;4&#39;, &#39;7&#39;, &#39;8&#39;, &#39;2&#39;, &#39;5&#39;, &#39;6&#39;, &#39;2&#39;, &#39;7&#39;, &#39;5&#39;, &#39;3&#39;]\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["As one last example of using `textFile()`, we will read the contents of a file containing the text of the novel \"A Tale of Two Cities\". We see that this file contains 15,797 lines of text."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9916a167-b49a-4c43-afde-fcb7973a11ad"}}},{"cell_type":"code","source":["totc = sc.textFile('/FileStore/tables/tale_of_two_cities.txt')\nprint(totc.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cb47a94-2bb5-4824-aa60-d2d4a443ae1c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">15797\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">15797\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We will now display the first 25 lines from this text file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7811aa75-57dd-42a9-aebd-70215036935a"}}},{"cell_type":"code","source":["first25 = totc.collect()[:25]\n#first25 = totc.take(25) \n# this is a better way to look at the first 25, I am only calculate the first 25 items that I need. versus the first one reads all the items in memory\n\nfor i, line in enumerate(first25):\n    print(f'Line {i+1:02}:  {line}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f095ba09-c547-40fc-a9b9-b72c45d59975"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Line 01:  A TALE OF TWO CITIES\nLine 02:  \nLine 03:  A STORY OF THE FRENCH REVOLUTION\nLine 04:  \nLine 05:  By Charles Dickens\nLine 06:  \nLine 07:  Book the First--Recalled to Life\nLine 08:  \nLine 09:  I. The Period\nLine 10:  \nLine 11:  It was the best of times,\nLine 12:  it was the worst of times,\nLine 13:  it was the age of wisdom,\nLine 14:  it was the age of foolishness,\nLine 15:  it was the epoch of belief,\nLine 16:  it was the epoch of incredulity,\nLine 17:  it was the season of Light,\nLine 18:  it was the season of Darkness,\nLine 19:  it was the spring of hope,\nLine 20:  it was the winter of despair,\nLine 21:  we had everything before us,\nLine 22:  we had nothing before us,\nLine 23:  we were all going direct to Heaven,\nLine 24:  we were all going direct the other way--\nLine 25:  in short, the period was so far like the present period, that some of\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Line 01:  A TALE OF TWO CITIES\nLine 02:  \nLine 03:  A STORY OF THE FRENCH REVOLUTION\nLine 04:  \nLine 05:  By Charles Dickens\nLine 06:  \nLine 07:  Book the First--Recalled to Life\nLine 08:  \nLine 09:  I. The Period\nLine 10:  \nLine 11:  It was the best of times,\nLine 12:  it was the worst of times,\nLine 13:  it was the age of wisdom,\nLine 14:  it was the age of foolishness,\nLine 15:  it was the epoch of belief,\nLine 16:  it was the epoch of incredulity,\nLine 17:  it was the season of Light,\nLine 18:  it was the season of Darkness,\nLine 19:  it was the spring of hope,\nLine 20:  it was the winter of despair,\nLine 21:  we had everything before us,\nLine 22:  we had nothing before us,\nLine 23:  we were all going direct to Heaven,\nLine 24:  we were all going direct the other way--\nLine 25:  in short, the period was so far like the present period, that some of\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"599f79c0-188e-46c4-ad68-5b209dd42fd4"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.1","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"06 - Introduction to RDDs","dashboards":[{"elements":[{"elementNUID":"a5b60506-b898-4106-ae55-c427f43775bc","guid":"02c37d48-5f32-4ee3-86d8-fd71370f4eb5","options":null,"position":{"x":12,"y":0,"height":12,"width":12,"z":null},"elementType":"command"},{"elementNUID":"e4c996f9-dac3-4f8b-a203-6f2232dea43c","guid":"5191fc8d-b846-4da9-b3dc-a39309765add","options":null,"position":{"x":0,"y":3,"height":6,"width":12,"z":null},"elementType":"command"},{"elementNUID":"30a26ed1-4545-473a-8cce-6f2714803c0f","guid":"c00915a5-0920-4c1a-b65e-60d43944a382","options":null,"position":{"x":12,"y":15,"height":4,"width":12,"z":null},"elementType":"command"},{"elementNUID":"8f2d964c-8ea5-4d90-ace1-e4f107fa9a9f","guid":"f9f9e68b-db15-49cb-bfac-f3ece95f0b77","options":null,"position":{"x":12,"y":19,"height":1,"width":12,"z":null},"elementType":"command"}],"guid":"58946aeb-46a3-4a63-a664-68360b629707","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"9d8da7e9-60cb-421a-a4e3-1407f8c02846","origId":2377394210003475,"title":"Untitled","width":1024,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2377394210003444}},"nbformat":4,"nbformat_minor":0}
